{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c73133af-7360-4762-9048-3aa0b3bb71b4",
   "metadata": {},
   "source": [
    "### Agenda:\n",
    "1. Data Loading & Preprocessing\n",
    "   - Missing values handling\n",
    "   - Date features creation\n",
    "   - Train/Test split\n",
    "   - Scaling\n",
    "   - Sequences\n",
    "   - Data Loader (incl. indexing for Basisformer)\n",
    "2. Experimental Design\n",
    "    - Benchmark Models\n",
    "      - Linear Regression\n",
    "      - LSTM\n",
    "    - Pre trained Chronos\n",
    "    - Transformers\n",
    "      - Non-Stationary Autoformer\n",
    "      - BasisFormer\n",
    "      - iTransformer\n",
    "3. Results\n",
    "4. Outlook\n",
    "   - Chronos Simulation Framework\n",
    "   - DYNOTEARS Causal Structure\n",
    "   - Non linear causal structure\n",
    "   - Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veljByFX557b",
   "metadata": {
    "id": "veljByFX557b"
   },
   "outputs": [],
   "source": [
    "#!pip install torch==2.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370ef93a-0cd3-471b-a6d1-1b3ed3da369c",
   "metadata": {},
   "source": [
    "# 1. Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c78b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad7a09c-a0e5-4eff-b76b-752fcb66665b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4de819",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.weight_norm as wn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a448b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import argparse\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c789d3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unification import unification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0948b2-e0b7-4862-9a96-8f186c1d4244",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iGtogtkxzGiT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "iGtogtkxzGiT",
    "outputId": "d63b1548-a355-4ac8-89ed-fc0029c44b31"
   },
   "outputs": [],
   "source": [
    "##file_path = '/content/all_countries.csv' ## colab path\n",
    "file_path = 'data/all_countries.csv' ## jupyter path\n",
    "df = pd.read_csv(file_path)\n",
    "df = pd.DataFrame(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vQW9jbwq4lFG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477
    },
    "id": "vQW9jbwq4lFG",
    "outputId": "e9aa57ef-18a2-4243-cd79-51b284512116"
   },
   "outputs": [],
   "source": [
    "df = df [['Country','Datetime (UTC)',  'Price (EUR/MWhe)']]\n",
    "df = df.pivot(index='Datetime (UTC)', columns='Country', values='Price (EUR/MWhe)')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mfQvgSnM3_1v",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mfQvgSnM3_1v",
    "outputId": "9cd1c2f9-c7f7-46d1-df8b-df5c13df26d9"
   },
   "outputs": [],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WObloVqL4aH3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WObloVqL4aH3",
    "outputId": "c1a5702e-1761-4cca-d65c-14966b09d8dd"
   },
   "outputs": [],
   "source": [
    "df = df.dropna(axis=1)\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AB6vyJX-C6h3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AB6vyJX-C6h3",
    "outputId": "aa9990f8-cb44-4367-b4ff-e2a1bf1732d7"
   },
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)\n",
    "df.columns.name = None\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99211c4c-9b8f-4d60-9bb6-add80102ca5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Datetime (UTC)'] = pd.to_datetime(df['Datetime (UTC)'])\n",
    "last_time_point = df['Datetime (UTC)'].max()\n",
    "print(\"Last time point available:\", last_time_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9365312-6296-4b97-ace6-9dd9191fccea",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2024-02-01 01:00:00' ## FILTERING ONLY FOR 3 MONTHS\n",
    "df = df[df['Datetime (UTC)'] >= pd.to_datetime(start_date)]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_Jtyfje24Pdq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Jtyfje24Pdq",
    "outputId": "507d28c6-0347-4446-9078-c6fda9ffa9a6"
   },
   "outputs": [],
   "source": [
    "df['month'] = df['Datetime (UTC)'].apply(lambda row: row.month)\n",
    "df['day'] = df['Datetime (UTC)'].apply(lambda row: row.day)\n",
    "df['weekday'] = df['Datetime (UTC)'].apply(lambda row: row.weekday())\n",
    "df['hour'] = df['Datetime (UTC)'].apply(lambda row: row.hour)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80mxqDAh5Mwd",
   "metadata": {
    "id": "80mxqDAh5Mwd"
   },
   "outputs": [],
   "source": [
    "# separating the electricity prices and timestamp features\n",
    "electricity_prices_df = df[['Datetime (UTC)', 'Austria', 'Belgium', 'Czechia', 'Denmark', 'Estonia', 'Finland', 'France',\n",
    "              'Germany', 'Greece', 'Hungary', 'Italy', 'Latvia', 'Lithuania', 'Luxembourg',\n",
    "             'Netherlands', 'Norway', 'Poland', 'Portugal', 'Romania', 'Slovakia',\n",
    "             'Slovenia', 'Spain', 'Sweden', 'Switzerland']]\n",
    "timestamp_features_df = df[['Datetime (UTC)', 'month', 'day', 'weekday', 'hour']]\n",
    "\n",
    "# defining the split ratio\n",
    "train_size = 0.8\n",
    "train_size_electricity = int(len(electricity_prices_df) * train_size)\n",
    "train_size_timestamp = int(len(timestamp_features_df) * train_size)\n",
    "\n",
    "# spliting the data into train and test sets\n",
    "electricity_prices_train = electricity_prices_df[:train_size_electricity]\n",
    "electricity_prices_test = electricity_prices_df[train_size_electricity:]\n",
    "timestamp_features_train = timestamp_features_df[:train_size_timestamp]\n",
    "timestamp_features_test = timestamp_features_df[train_size_timestamp:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95180c89-5259-40f8-af8b-e90241aa8864",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(electricity_prices_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2c3f05-460c-4d80-868e-48ff8f5cfd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#country_names = electricity_prices_train.drop(columns=['Datetime (UTC)']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4oRyT6un5WWm",
   "metadata": {
    "id": "4oRyT6un5WWm"
   },
   "outputs": [],
   "source": [
    "# rescaling the electricity prices\n",
    "#scaler = StandardScaler()\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "electricity_prices_train_scaled = scaler.fit_transform(electricity_prices_train.drop(columns=['Datetime (UTC)']))\n",
    "electricity_prices_test_scaled = scaler.transform(electricity_prices_test.drop(columns=['Datetime (UTC)']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241b9b80-b3c1-423c-a0ac-0e100fe0858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_count = np.sum(electricity_prices_train_scaled == 0)\n",
    "print(f\"Number of zero values in actual data: {zero_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d72a8e-4df2-4255-bdb3-3353426389cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#electricity_prices_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407a011b-3187-4e94-9c3a-e36c17253639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, seq_length, pred_length, label_length, curr_model):\n",
    "    seq_x = [] # storing for input seqiences\n",
    "    seq_y = [] # storing for output seqiences\n",
    "    for i in range(len(data) - seq_length - pred_length):\n",
    "        seq_x.append(data[i:i+seq_length])\n",
    "        if curr_model in [\"basis_former\", \"itransformer\", \"ns_autoformer\"]:\n",
    "          seq_y.append(data[i+seq_length-label_length:i+seq_length+pred_length])\n",
    "        else: ## only chronos\n",
    "          seq_y.append(data[i+seq_length:i+seq_length+pred_length])\n",
    "    return np.array(seq_x), np.array(seq_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c17fd64-77c4-42ab-9e63-bf919d801d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(seq_x, seq_y, seq_x_mark, seq_y_mark, batch_size, curr_model):\n",
    "    seq_x = torch.tensor(seq_x, dtype=torch.float32)\n",
    "    seq_y = torch.tensor(seq_y, dtype=torch.float32)\n",
    "    seq_x_mark = torch.tensor(seq_x_mark, dtype=torch.float32)\n",
    "    seq_y_mark = torch.tensor(seq_y_mark, dtype=torch.float32)\n",
    "    \n",
    "    if curr_model == \"basis_former\":\n",
    "        indices = []\n",
    "        total_len = len(seq_x)\n",
    "        for i in range(total_len):\n",
    "            index_list = np.arange(i, i + len(seq_x[0]) + len(seq_y[0]), 1)\n",
    "            norm_index = index_list / total_len\n",
    "            indices.append(norm_index)\n",
    "        indices = torch.tensor(indices, dtype=torch.float32)\n",
    "        dataset = TensorDataset(seq_x, seq_y, seq_x_mark, seq_y_mark, indices)\n",
    "    else:\n",
    "        dataset = TensorDataset(seq_x, seq_y, seq_x_mark, seq_y_mark)\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=2, shuffle=True, drop_last=True)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21225b3d-5c69-425f-9f8e-17ebcc39cd8b",
   "metadata": {},
   "source": [
    "# 2. Experimental Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6a0895",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 96\n",
    "pred_length = 48\n",
    "label_length = 48\n",
    "batch_size = 24\n",
    "device = torch.device(f\"cuda:{args.device}\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9598752-60c2-45e0-a3da-535bb910219e",
   "metadata": {},
   "source": [
    "# Benchmark Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1627bd8d-ab8f-4f35-81c8-25b91760b230",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c10844",
   "metadata": {},
   "source": [
    "### Linear Model Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad38e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the main dataframe for linear model preprocessing\n",
    "df_linear = df.copy()\n",
    "\n",
    "# Define the number of lagged features\n",
    "lag_steps = 3\n",
    "\n",
    "# List of all columns (already defined in the main preprocessing)\n",
    "all_columns = df_linear.columns.tolist()\n",
    "\n",
    "# List of columns to exclude (non-country columns)\n",
    "exclude_columns = ['Datetime (UTC)', 'month', 'day', 'weekday', 'hour']\n",
    "\n",
    "# Define the country columns by excluding non-country columns\n",
    "countries = [col for col in all_columns if col not in exclude_columns]\n",
    "\n",
    "# Create lagged features for each country\n",
    "for country in countries:\n",
    "    for lag in range(1, lag_steps + 1):\n",
    "        df_linear[f'{country}_lag_{lag}'] = df_linear[country].shift(lag)\n",
    "\n",
    "# Drop rows with NaN values due to lagging\n",
    "df_linear.dropna(inplace=True)\n",
    "\n",
    "# Define features (X) and targets (Y)\n",
    "X_numerical = df_linear.drop(columns=countries + ['Datetime (UTC)'])\n",
    "Y = df_linear[countries]  # Target: current prices of all countries\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_numerical)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_size = 0.8\n",
    "train_size_idx = int(len(X_scaled) * train_size)\n",
    "X_train, X_test = X_scaled[:train_size_idx], X_scaled[train_size_idx:]\n",
    "Y_train, Y_test = Y[:train_size_idx], Y[train_size_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c39d0c",
   "metadata": {},
   "source": [
    "### Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575df258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Initialize and train the linear regression model\n",
    "model_lr = LinearRegression()\n",
    "model_lr.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "Y_pred_lr = model_lr.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_lr = mean_squared_error(Y_test, Y_pred_lr)\n",
    "print(f\"Linear Regression Mean Squared Error: {mse_lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb17613-c608-4a1b-b8dd-7773464d72b1",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07edd48",
   "metadata": {},
   "source": [
    "### LSTM Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740f3adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Copy the main dataframe for LSTM model preprocessing\n",
    "df_lstm = df.copy()\n",
    "\n",
    "# Rescale the data using StandardScaler for LSTM\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(df_lstm.drop(columns=['Datetime (UTC)', 'month', 'day', 'weekday', 'hour']))\n",
    "\n",
    "# Convert to a supervised learning problem by creating sequences\n",
    "def create_sequences(data, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i+seq_length])\n",
    "        y.append(data[i+seq_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Define sequence length (number of time steps)\n",
    "seq_length = 24\n",
    "\n",
    "# Create sequences\n",
    "X, y = create_sequences(data_scaled, seq_length)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_size = int(X.shape[0] * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b871c7e",
   "metadata": {},
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd87f6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define the LSTM model\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(units=50, return_sequences=True, input_shape=(seq_length, X.shape[2])))\n",
    "model_lstm.add(LSTM(units=50, return_sequences=False))\n",
    "model_lstm.add(Dense(units=y.shape[1]))\n",
    "\n",
    "# Compile the model\n",
    "model_lstm.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Fit the model\n",
    "model_lstm.fit(X_train, y_train, epochs=1, batch_size=24, validation_data=(X_test, y_test))\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lstm = model_lstm.predict(X_test)\n",
    "\n",
    "# Inverse transform the scaled data to original values\n",
    "y_test_inverse = scaler.inverse_transform(y_test)\n",
    "y_pred_inverse = scaler.inverse_transform(y_pred_lstm)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_lstm = mean_squared_error(y_test_inverse, y_pred_inverse)\n",
    "print(f\"LSTM Mean Squared Error: {mse_lstm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf856e7",
   "metadata": {},
   "source": [
    "### Improved LSTM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09eb0b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define an improved LSTM model\n",
    "model_lstm = Sequential()\n",
    "\n",
    "# Increase the number of units in LSTM layers\n",
    "model_lstm.add(LSTM(units=64, return_sequences=True, input_shape=(seq_length, X.shape[2])))\n",
    "model_lstm.add(Dropout(0.2))  # Add dropout for regularization\n",
    "\n",
    "model_lstm.add(LSTM(units=64, return_sequences=False))\n",
    "model_lstm.add(Dropout(0.2))  # Add dropout for regularization\n",
    "\n",
    "# Output layer\n",
    "model_lstm.add(Dense(units=y.shape[1]))\n",
    "\n",
    "# Compile the model with a slightly lower learning rate\n",
    "model_lstm.compile(optimizer=Adam(learning_rate=0.0005), loss='mean_squared_error')\n",
    "\n",
    "# Early stopping to prevent overfitting and reduce learning rate on plateau\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001)\n",
    "\n",
    "# Fit the model\n",
    "model_lstm.fit(X_train, y_train, epochs=1, batch_size=24, \n",
    "               validation_data=(X_test, y_test), \n",
    "               callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lstm = model_lstm.predict(X_test)\n",
    "\n",
    "# Inverse transform the scaled data to original values\n",
    "y_test_inverse = scaler.inverse_transform(y_test)\n",
    "y_pred_inverse = scaler.inverse_transform(y_pred_lstm)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_lstm = mean_squared_error(y_test_inverse, y_pred_inverse)\n",
    "print(f\"Improved LSTM Mean Squared Error: {mse_lstm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524dced6-ac13-4939-9362-ab95493547d0",
   "metadata": {},
   "source": [
    "# Pre trained model Chronos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133a20c4-aa7e-4fd7-95e7-8e60275dc839",
   "metadata": {},
   "source": [
    "zero shot evaluation with Chronos Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2617ec8b-f710-450b-81b1-1f8ce6f4fba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/amazon-science/chronos-forecasting.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4e2e76-e80a-45da-aba7-c82e5dae03b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chronos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from chronos import ChronosPipeline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df is your initial dataset\n",
    "# Make sure 'Datetime (UTC)' is set as the index\n",
    "#df = df.set_index('Datetime (UTC)')\n",
    "\n",
    "# Select the countries we want to forecast\n",
    "selected_countries = ['Austria', 'Belgium', 'Czechia', 'Denmark', 'Estonia', 'Finland', 'France',\n",
    "              'Germany', 'Greece', 'Hungary', 'Italy', 'Latvia', 'Lithuania', 'Luxembourg',\n",
    "             'Netherlands', 'Norway', 'Poland', 'Portugal', 'Romania', 'Slovakia',\n",
    "             'Slovenia', 'Spain', 'Sweden', 'Switzerland']\n",
    "\n",
    "# Extract the data for selected countries\n",
    "data = df[selected_countries]\n",
    "\n",
    "# Print some basic information about the dataset\n",
    "#print(f\"Dataset shape: {data.shape}\")\n",
    "#print(f\"Date range: from {data.index.min()} to {data.index.max()}\")\n",
    "#print(f\"Any missing values: {data.isnull().any().any()}\")\n",
    "\n",
    "# Define the split point for train and test\n",
    "split_point = int(len(data) * 0.8)  # 80% for training, 20% for testing\n",
    "\n",
    "# Split the data\n",
    "train_data = data.iloc[:split_point]\n",
    "test_data = data.iloc[split_point:]\n",
    "\n",
    "#print(f\"\\nTrain data shape: {train_data.shape}\")\n",
    "#print(f\"Test data shape: {test_data.shape}\")\n",
    "\n",
    "# Function to prepare data for Chronos\n",
    "def prepare_chronos_data(data, seq_length):\n",
    "    return torch.tensor(data[-seq_length:].values, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "# Initialize Chronos pipeline\n",
    "pipeline = ChronosPipeline.from_pretrained(\n",
    "    \"amazon/chronos-t5-small\",\n",
    "    device_map=\"cpu\",  # use \"cpu\" for CPU inference and \"mps\" for Apple Silicon\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Set parameters\n",
    "seq_length = 96\n",
    "pred_length = 48\n",
    "\n",
    "#print(f\"\\nSequence length: {seq_length} (equivalent to {seq_length/24} days)\")\n",
    "#print(f\"Prediction length: {pred_length} (equivalent to {pred_length/24} days)\")\n",
    "\n",
    "# Prepare data and run forecasts\n",
    "results = {}\n",
    "\n",
    "for country in selected_countries:\n",
    "    print(f\"\\nProcessing {country}\")\n",
    "    print(f\"Data range for {country}: {train_data[country].min()} to {train_data[country].max()}\")\n",
    "    \n",
    "    # Prepare input for Chronos\n",
    "    chronos_input = prepare_chronos_data(train_data[country], seq_length)\n",
    "    print(f\"Chronos input shape: {chronos_input.shape}\")\n",
    "    \n",
    "    # Generate forecast\n",
    "    forecast = pipeline.predict(\n",
    "        context=chronos_input,\n",
    "        prediction_length=pred_length,\n",
    "        num_samples=20,\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    results[country] = forecast\n",
    "    print(f\"Forecast shape: {forecast.shape}\")\n",
    "\n",
    "\n",
    "# Evaluate results\n",
    "def calculate_mse(actual, forecast):\n",
    "    return np.mean((actual - forecast) ** 2)\n",
    "    \n",
    "def calculate_mae(actual, forecast):\n",
    "    return np.mean(np.abs(actual - forecast))\n",
    "    \n",
    "def calculate_rmse(actual, forecast):\n",
    "    return np.sqrt(calculate_mse(forecast, actual))\n",
    "    \n",
    "def calculate_mape(actual, forecast):\n",
    "    return np.mean(np.abs((forecast - actual) / actual))\n",
    "\n",
    "mse_results = {}\n",
    "mae_results = {}\n",
    "mape_results = {}\n",
    "rmse_results = {}\n",
    "\n",
    "for country in selected_countries:\n",
    "    actual_values = test_data[country].values[:pred_length]\n",
    "    forecasted_values = np.median(results[country], axis=1).flatten()\n",
    "    \n",
    "    min_length = min(len(actual_values), len(forecasted_values))\n",
    "    actual_values = actual_values[:min_length]\n",
    "    forecasted_values = forecasted_values[:min_length]\n",
    "    \n",
    "    mse = calculate_mse(actual_values, forecasted_values)\n",
    "    mae = calculate_mae(actual_values, forecasted_values)\n",
    "    rmse = calculate_rmse(actual_values, forecasted_values)\n",
    "    mape = calculate_mape(actual_values, forecasted_values)\n",
    "    mse_results[country] = mse\n",
    "    mae_results[country] = mae\n",
    "    rmse_results[country] = rmse\n",
    "    mape_results[country] = mape\n",
    "    print(f\"\\nMSE for {country}: {mse}\")\n",
    "    print(f\"MAE for {country}: {mae}\")\n",
    "    print(f\"\\nRMSE for {country}: {rmse}\")\n",
    "    print(f\"MAPE for {country}: {mape}\")\n",
    "    \n",
    "    # Debug information\n",
    "    #print(f\"Actual values shape: {actual_values.shape}\")\n",
    "    #print(f\"Forecasted values shape: {forecasted_values.shape}\")\n",
    "    #print(f\"Actual values range: {actual_values.min()} to {actual_values.max()}\")\n",
    "    #print(f\"Forecasted values range: {forecasted_values.min()} to {forecasted_values.max()}\")\n",
    "    \n",
    "    # Plot actual vs predicted\n",
    "    #plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot actual values\n",
    "    #if len(actual_values) == 1:\n",
    "    #    plt.scatter(0, actual_values[0], label='Actual', marker='o', s=100, color='blue')\n",
    "    #else:\n",
    "    #    plt.plot(actual_values, label='Actual', marker='o')\n",
    "    \n",
    "    # Plot forecasted values\n",
    "    #plt.plot(forecasted_values, label='Predicted', marker='x', color='red')\n",
    "    \n",
    "    #plt.title(f'{country} - Actual vs Predicted')\n",
    "    #plt.xlabel('Time Steps')\n",
    "    #plt.ylabel('Price')\n",
    "    #plt.legend()\n",
    "    #plt.grid(True)\n",
    "    \n",
    "    # Add text annotations\n",
    "    #if len(actual_values) > 0:\n",
    "    #    plt.annotate(f'{actual_values[0]:.2f}', (0, actual_values[0]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "    #if len(actual_values) > 1:\n",
    "    #    plt.annotate(f'{actual_values[-1]:.2f}', (len(actual_values)-1, actual_values[-1]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "    \n",
    "    #plt.annotate(f'{forecasted_values[0]:.2f}', (0, forecasted_values[0]), textcoords=\"offset points\", xytext=(0,-15), ha='center')\n",
    "    #plt.annotate(f'{forecasted_values[-1]:.2f}', (len(forecasted_values)-1, forecasted_values[-1]), textcoords=\"offset points\", xytext=(0,-15), ha='center')\n",
    "    \n",
    "    #plt.tight_layout()\n",
    "    #plt.show()\n",
    "\n",
    "# Calculate and print average MSE\n",
    "average_mse = np.mean(list(mse_results.values()))\n",
    "print(f\"\\nAverage MSE across selected countries: {average_mse}\")\n",
    "average_mae = np.mean(list(mae_results.values()))\n",
    "print(f\"\\nAverage MAE across selected countries: {average_mae}\")\n",
    "average_rmse = np.mean(list(rmse_results.values()))\n",
    "print(f\"\\nAverage RMSE across selected countries: {average_rmse}\")\n",
    "average_mape = np.mean(list(mape_results.values()))\n",
    "print(f\"\\nAverage MAPE across selected countries: {average_mape}\")\n",
    "\n",
    "# Print summary statistics\n",
    "#for country in selected_countries:\n",
    "    #print(f\"\\nSummary for {country}:\")\n",
    "    #print(f\"Train data mean: {train_data[country].mean():.2f}, std: {train_data[country].std():.2f}\")\n",
    "    #print(f\"Test data mean: {test_data[country].mean():.2f}, std: {test_data[country].std():.2f}\")\n",
    "    #print(f\"MSE: {mse_results[country]:.2f}\")\n",
    "    #print(f\"RMSE: {np.sqrt(mse_results[country]):.2f}\")\n",
    "    #print(f\"MAE: {mae_results[country]:.2f}\")\n",
    "    #print(f\"RMSE as percentage of mean: {(np.sqrt(mse_results[country]) / test_data[country].mean()) * 100:.2f}%\")\n",
    "    #print(f\"MAE as percentage of mean: {(mae_results[country] / test_data[country].mean()) * 100:.2f}%\")\n",
    "\n",
    "# Save MSE results\n",
    "#import pickle\n",
    "#with open('error_results_chronos_unscaled.pkl', 'wb') as f:\n",
    "    #pickle.dump({'mse': mse_results, 'mae': mae_results}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5a7ac4-ef50-4568-aeb2-ed1cec9d015f",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ee0fbc-fd5f-4b8e-99f0-e0d05ff0826d",
   "metadata": {},
   "source": [
    "## Basisformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ILk9LX4k5Yuu",
   "metadata": {
    "id": "ILk9LX4k5Yuu"
   },
   "outputs": [],
   "source": [
    "seq_length = 96\n",
    "pred_length = 48\n",
    "label_length = 48\n",
    "curr_model = \"basis_former\"\n",
    "\n",
    "train_seq_x, train_seq_y = create_sequences(electricity_prices_train_scaled, seq_length, pred_length, label_length, curr_model)\n",
    "train_seq_x_mark, train_seq_y_mark = create_sequences(timestamp_features_train.drop(columns=['Datetime (UTC)']).values, seq_length, pred_length, label_length, curr_model)\n",
    "\n",
    "\n",
    "test_seq_x, test_seq_y = create_sequences(electricity_prices_test_scaled, seq_length, pred_length,label_length, curr_model)\n",
    "test_seq_x_mark, test_seq_y_mark = create_sequences(timestamp_features_test.drop(columns=['Datetime (UTC)']).values, seq_length, pred_length, label_length, curr_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IeE-jvzOEVw-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IeE-jvzOEVw-",
    "outputId": "d4778504-79d4-475b-c868-17b3994fd3fc"
   },
   "outputs": [],
   "source": [
    "print(\"Sample training sequence x:\", train_seq_x[0])\n",
    "print(\"Sample training sequence y:\", train_seq_y[0])\n",
    "print(\"Sample training sequence x mark:\", train_seq_x_mark[0])\n",
    "print(\"Sample training sequence y mark:\", train_seq_y_mark[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OrL7qSyl5bqB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OrL7qSyl5bqB",
    "outputId": "01496ef4-4d84-4630-dc4b-5a1b9c36a404"
   },
   "outputs": [],
   "source": [
    "batch_size = 24\n",
    "\n",
    "# converting sequences to PyTorch DataLoader objects\n",
    "train_loader = create_dataloader(train_seq_x, train_seq_y, train_seq_x_mark, train_seq_y_mark, batch_size, curr_model)\n",
    "test_loader = create_dataloader(test_seq_x, test_seq_y, test_seq_x_mark, test_seq_y_mark, batch_size, curr_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca8679b-4ac1-42ab-8065-f6f2519e40cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    print(f\"Batch contains {len(batch)} items:\")\n",
    "    for i, item in enumerate(batch):\n",
    "        print(f\"Item {i}: Shape = {item.shape if torch.is_tensor(item) else 'Not a tensor'}\")\n",
    "    break  # Just print the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Des1ClFuevfI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Des1ClFuevfI",
    "outputId": "538a4886-5959-4a8c-a4e8-3239df3ed5ec"
   },
   "outputs": [],
   "source": [
    "##pip install adabelief_pytorch==0.2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c7af02-3c5d-4a35-a1e1-ed6266bed132",
   "metadata": {},
   "source": [
    "### Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f18f708-9681-4f6a-b70b-0ad9009722f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import Basisformer.model\n",
    "importlib.reload(Basisformer.model)\n",
    "from Basisformer.model import Basisformer\n",
    "\n",
    "import Basisformer.main\n",
    "importlib.reload(Basisformer.main)\n",
    "from Basisformer.main import parse_args, model_setup, log_and_print\n",
    "\n",
    "args = parse_args()\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(f\"cuda:{args.device}\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set up model\n",
    "model = model_setup(args, device)\n",
    "\n",
    "# Log arguments and model\n",
    "log_and_print('Args in experiment:')\n",
    "log_and_print(args)\n",
    "log_and_print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d324502",
   "metadata": {},
   "source": [
    "## Basisformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921c5794",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_model = \"basisformer\"  #\"basis_former\", \"itransformer\", \"ns_autoformer\"\n",
    "\n",
    "train_seq_x, train_seq_y = create_sequences(electricity_prices_train_scaled, seq_length, pred_length, label_length, curr_model)\n",
    "train_seq_x_mark, train_seq_y_mark = create_sequences(timestamp_features_train.drop(columns=['Datetime (UTC)']).values, seq_length, pred_length, label_length, curr_model)\n",
    "\n",
    "\n",
    "test_seq_x, test_seq_y = create_sequences(electricity_prices_test_scaled, seq_length, pred_length,label_length, curr_model)\n",
    "test_seq_x_mark, test_seq_y_mark = create_sequences(timestamp_features_test.drop(columns=['Datetime (UTC)']).values, seq_length, pred_length, label_length, curr_model)\n",
    "\n",
    "\n",
    "# converting sequences to PyTorch DataLoader objects\n",
    "train_loader = create_dataloader(train_seq_x, train_seq_y, train_seq_x_mark, train_seq_y_mark, batch_size, curr_model)\n",
    "test_loader = create_dataloader(test_seq_x, test_seq_y, test_seq_x_mark, test_seq_y_mark, batch_size, curr_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25388088-0a01-4a53-9327-36baeb5ec317",
   "metadata": {},
   "source": [
    "### Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d34df7-21cb-4303-87f9-d5ec8c03ec8e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33d34df7-21cb-4303-87f9-d5ec8c03ec8e",
    "outputId": "3c5fe222-fb0a-4abc-d84a-489ef41b44b1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#curr_model = \"basis_former\" \n",
    "basisformer_train_test = unification.fit_transformer(model=curr_model, train_flag=True, test_flag=True, train_loader=train_loader, test_loader=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ffc309-339b-43e4-ab87-1e1ece074e45",
   "metadata": {},
   "source": [
    "## iTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf6cf6e-e6dc-41fd-a14e-db5a55912360",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_model = \"itransformer\"  #\"basis_former\", \"itransformer\", \"ns_autoformer\"\n",
    "\n",
    "train_seq_x, train_seq_y = create_sequences(electricity_prices_train_scaled, seq_length, pred_length, label_length, curr_model)\n",
    "train_seq_x_mark, train_seq_y_mark = create_sequences(timestamp_features_train.drop(columns=['Datetime (UTC)']).values, seq_length, pred_length, label_length, curr_model)\n",
    "\n",
    "\n",
    "test_seq_x, test_seq_y = create_sequences(electricity_prices_test_scaled, seq_length, pred_length,label_length, curr_model)\n",
    "test_seq_x_mark, test_seq_y_mark = create_sequences(timestamp_features_test.drop(columns=['Datetime (UTC)']).values, seq_length, pred_length, label_length, curr_model)\n",
    "\n",
    "\n",
    "# converting sequences to PyTorch DataLoader objects\n",
    "train_loader = create_dataloader(train_seq_x, train_seq_y, train_seq_x_mark, train_seq_y_mark, batch_size, curr_model)\n",
    "test_loader = create_dataloader(test_seq_x, test_seq_y, test_seq_x_mark, test_seq_y_mark, batch_size, curr_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca24c83-5c85-41d7-adfb-4b679d9bb9cf",
   "metadata": {},
   "source": [
    "### Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5207bb8-dd92-41b0-9f62-eb436fb7e4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "itransformer_train_test = unification.fit_transformer(model=curr_model, train_flag=True, test_flag=True, train_loader=train_loader, test_loader=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa62ae11-381d-4aaf-9e32-ade675824eeb",
   "metadata": {},
   "source": [
    "## Nonstationary Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a8451f-0280-4b1d-83f4-645080a0ebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_model = \"ns_autoformer\"\n",
    "\n",
    "train_seq_x, train_seq_y = create_sequences(electricity_prices_train_scaled, seq_length, pred_length, label_length, curr_model)\n",
    "train_seq_x_mark, train_seq_y_mark = create_sequences(timestamp_features_train.drop(columns=['Datetime (UTC)']).values, seq_length, pred_length, label_length, curr_model)\n",
    "\n",
    "\n",
    "test_seq_x, test_seq_y = create_sequences(electricity_prices_test_scaled, seq_length, pred_length,label_length, curr_model)\n",
    "test_seq_x_mark, test_seq_y_mark = create_sequences(timestamp_features_test.drop(columns=['Datetime (UTC)']).values, seq_length, pred_length, label_length, curr_model)\n",
    "\n",
    "\n",
    "# converting sequences to PyTorch DataLoader objects\n",
    "train_loader = create_dataloader(train_seq_x, train_seq_y, train_seq_x_mark, train_seq_y_mark, batch_size, curr_model)\n",
    "test_loader = create_dataloader(test_seq_x, test_seq_y, test_seq_x_mark, test_seq_y_mark, batch_size, curr_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a2769b",
   "metadata": {},
   "source": [
    "### Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55134ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_autoformer_train_test = unification.fit_transformer(model=curr_model, train_flag=True, test_flag=True, train_loader=train_loader, test_loader=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2803d193-eb4d-4ba8-b719-e2fa360e4167",
   "metadata": {},
   "source": [
    "# 3. Results - Models Performance Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0bbe9e-2ecc-49c0-bf7b-356fb6f80758",
   "metadata": {},
   "source": [
    "# 6. Outlook "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0876475-7871-4b24-b71c-4d70f4d5adea",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Chronos Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56344b93-c9be-4c74-a0af-028dfdd20cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"chronos[training] @ git+https://github.com/amazon-science/chronos-forecasting.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb5bb54-ba52-4564-9775-05648c81013c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python supporting_files_chronos/kernel-synth.py --num-series 500 --max-kernels 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c174fb6-a8cb-4ffb-b5fd-09a6d7908d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.ipc as ipc\n",
    "\n",
    "file_path = 'supporting_files_chronos/kernelsynth-data.arrow'\n",
    "\n",
    "with open(file_path, 'rb') as f:\n",
    "    reader = ipc.RecordBatchFileReader(f)\n",
    "    table = reader.read_all()\n",
    "\n",
    "df_ch = table.to_pandas()\n",
    "\n",
    "print(df_ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961121e6-2507-475c-8a80-7d6eb4489466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Number of time series\n",
    "num_series = 15\n",
    "# Number of plots per row\n",
    "plots_per_row = 5\n",
    "# Number of rows\n",
    "num_rows = (num_series + plots_per_row - 1) // plots_per_row\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, plots_per_row, figsize=(15, num_rows * 3))\n",
    "\n",
    "for i in range(num_series):\n",
    "    row = i // plots_per_row\n",
    "    col = i % plots_per_row\n",
    "    ax = axes[row, col]\n",
    "    ax.plot(df_ch['target'].iloc[i])\n",
    "    ax.set_title(f'Time Series {i}')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Value')\n",
    "\n",
    "# Remove any empty subplots\n",
    "for j in range(i + 1, num_rows * plots_per_row):\n",
    "    fig.delaxes(axes.flatten()[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9102227-5e5b-43ca-886e-362609622ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python supporting_files_chronos/kernel-synth-mult.py --num-series 500 --max-kernels 2 --dimensions 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c8baf1-c81a-4d00-a358-68f3697deda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.ipc as ipc\n",
    "\n",
    "file_path = 'supporting_files_chronos/kernelsynth-data.arrow'\n",
    "\n",
    "with open(file_path, 'rb') as f:\n",
    "    reader = ipc.RecordBatchFileReader(f)\n",
    "    table = reader.read_all()\n",
    "\n",
    "df_ch_mult = table.to_pandas()\n",
    "\n",
    "print(df_ch_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2f68c5-3899-42e3-a491-fae38480a6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_ch_mult.head())\n",
    "print(df_ch_mult['target'].head().apply(lambda x: np.array(x).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fb2bce-e5b5-4f44-b2f7-76815d8a78d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot multivariate time series\n",
    "def plot_multivariate_time_series(data, num_rows=3, num_cols=5):\n",
    "    num_series = num_rows * num_cols\n",
    "    time_points = np.arange(len(data[0]) // 3)  # 1024 time points for reshaped data\n",
    "    \n",
    "    fig, axs = plt.subplots(num_rows, num_cols, figsize=(20, num_rows * 4))\n",
    "    axs = axs.flatten()  # Flatten to easily iterate over subplots\n",
    "    \n",
    "    for i in range(num_series):\n",
    "        series = np.array(data[i]).reshape(-1, 3)  # Reshape to [1024, 3]\n",
    "        for j in range(series.shape[1]):\n",
    "            axs[i].plot(time_points, series[:, j], label=f'Dimension {j+1}')\n",
    "        axs[i].set_title(f'Time Series {i+1}')\n",
    "        axs[i].set_xlabel('Time')\n",
    "        axs[i].set_ylabel('Value')\n",
    "        axs[i].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Extract the 'target' column as a list and plot the first 15 multivariate time series\n",
    "plot_multivariate_time_series(df_ch_mult['target'].head(15).tolist(), num_rows=3, num_cols=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8868e96d-c11f-44da-8284-5e3bdd7aa438",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## DYNOTEARS Causal Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a06305-e94e-4e3e-8c8a-5a5036bf69e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from causalnex.structure.notears import from_pandas\n",
    "df_str = df.drop(columns=['Datetime (UTC)'])\n",
    "sm = from_pandas(df_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b7a18a-5c45-4440-b616-9c6f949c4db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from causalnex.plots import plot_structure, NODE_STYLE, EDGE_STYLE\n",
    "\n",
    "viz = plot_structure(\n",
    "    sm,\n",
    "    all_node_attributes=NODE_STYLE.WEAK,\n",
    "    all_edge_attributes=EDGE_STYLE.WEAK,\n",
    ")\n",
    "\n",
    "viz.toggle_physics(False)\n",
    "viz.show(\"supporting_files_dynotears/01_fully_connected.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297ebd63-03d7-45ab-8815-f4417d6ffeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.remove_edges_below_threshold(0.8)\n",
    "viz = plot_structure(\n",
    "    sm,\n",
    "    all_node_attributes=NODE_STYLE.WEAK,\n",
    "    all_edge_attributes=EDGE_STYLE.WEAK,\n",
    ")\n",
    "viz.show(\"supporting_files_dynotears/01_thresholded.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed52dbcf-3b38-457a-b105-b536abcccf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from causalnex.structure.notears import from_pandas\n",
    "df_str = df.drop(columns=['Datetime (UTC)'])\n",
    "sm = from_pandas(df_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad28f5e0-39c1-4129-a872-fa77fc837671",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.remove_edges_below_threshold(0.8)\n",
    "viz = plot_structure(\n",
    "    sm,\n",
    "    all_node_attributes=NODE_STYLE.WEAK,\n",
    "    all_edge_attributes=EDGE_STYLE.WEAK,\n",
    ")\n",
    "viz.show(\"supporting_files_dynotears/01_thresholded.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27208a4-9616-40ff-b2f3-d4455498b979",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Granger causality test with nonlinear forecasting methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71002567-98ca-49d4-a3d1-eb9dc74bf455",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nonlincausality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a85194-ab4f-4f57-b164-abe8bce5091d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Feb  7 23:29:32 2022\n",
    "\n",
    "@author: Maciej Rosoł\n",
    "\n",
    "contact: mrosol5@gmail.com, maciej.rosol.dokt@pw.edu.pl\n",
    "\"\"\"\n",
    "#%%\n",
    "import os\n",
    "\n",
    "# os.chdir(os.path.dirname(__file__))\n",
    "import numpy as np\n",
    "##import tensorflow\n",
    "import nonlincausality as nlc\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from nonlincausality.utils import prepare_data_for_prediction, calculate_pred_and_errors\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae4f41d-d94a-4877-987a-206fe905892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Data generation Y->X\n",
    "np.random.seed(10)\n",
    "y = (\n",
    "    np.cos(np.linspace(0, 20, 10_100))\n",
    "    + np.sin(np.linspace(0, 3, 10_100))\n",
    "    - 0.2 * np.random.random(10_100)\n",
    ")\n",
    "np.random.seed(20)\n",
    "x = 2 * y ** 3 - 5 * y ** 2 + 0.3 * y + 2 - 0.05 * np.random.random(10_100)\n",
    "data = np.vstack([x[:-100], y[100:]]).T\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(data[:, 0], label=\"X\")\n",
    "plt.plot(data[:, 1], label=\"Y\")\n",
    "plt.xlabel(\"Number of sample\")\n",
    "plt.ylabel(\"Signals [AU]\")\n",
    "plt.legend()\n",
    "\n",
    "#%% Test in case of presence of the causality\n",
    "lags = [50, 150]\n",
    "data_train = data[:6000, :]\n",
    "data_val = data[6000:8000, :]\n",
    "data_test = data[8000:, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8f4823-6810-4a83-941e-b759cddef251",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = nlc.nonlincausalityNN(\n",
    "    x=data_train,\n",
    "    maxlag=lags,\n",
    "    NN_config=['d','dr','d','dr'],\n",
    "    NN_neurons=[100,0.05,100,0.05],\n",
    "    x_test=data_test,\n",
    "    run=3,\n",
    "    epochs_num=[50, 50],\n",
    "    learning_rate=[0.0001, 0.00001],\n",
    "    batch_size_num=32,\n",
    "    x_val=data_val,\n",
    "    reg_alpha=None,\n",
    "    callbacks=None,\n",
    "    verbose=True,\n",
    "    plot=True,\n",
    ")\n",
    "\n",
    "#%% Example of obtaining the results\n",
    "for lag in lags:\n",
    "    best_model_X = results[lag].best_model_X\n",
    "    best_model_XY = results[lag].best_model_XY\n",
    "\n",
    "    p_value = results[lag].p_value\n",
    "    test_statistic = results[lag]._test_statistic\n",
    "\n",
    "    best_history_X = results[lag].best_history_X\n",
    "    best_history_XY = results[lag].best_history_XY\n",
    "\n",
    "    nlc.plot_history_loss(best_history_X, best_history_XY)\n",
    "    plt.title(\"Lag = %d\" % lag)\n",
    "    best_errors_X = results[lag].best_errors_X\n",
    "    best_errors_XY = results[lag].best_errors_XY\n",
    "\n",
    "    cohens_d = np.abs(\n",
    "        (np.mean(np.abs(best_errors_X)) - np.mean(np.abs(best_errors_XY)))\n",
    "        / np.std([best_errors_X, best_errors_XY])\n",
    "    )\n",
    "    print(\"For lag = %d Cohen's d = %0.3f\" % (lag, cohens_d))\n",
    "    print(f\"Test statistic = {test_statistic} p-value = {p_value}\")\n",
    "\n",
    "    # Using models for prediction\n",
    "    data_X, data_XY = prepare_data_for_prediction(data_test, lag)\n",
    "    X_pred_X = best_model_X.predict(data_X)\n",
    "    X_pred_XY = best_model_XY.predict(data_XY)\n",
    "\n",
    "    # Plot of true X vs X predicted\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    ax[0].plot(data_test[lag:, 0], X_pred_X, \"o\")\n",
    "    ax[0].set_xlabel(\"X test values\")\n",
    "    ax[0].set_ylabel(\"Predicted X values\")\n",
    "    ax[0].set_title(\"Model based on X\")\n",
    "    ax[1].plot(data_test[lag:, 0], X_pred_XY, \"o\")\n",
    "    ax[1].set_xlabel(\"X test values\")\n",
    "    ax[1].set_ylabel(\"Predicted X values\")\n",
    "    ax[1].set_title(\"Model based on X and Y\")\n",
    "    plt.suptitle(\"Lag = %d\" % lag)\n",
    "\n",
    "    # Another way of obtaining predicted values (and errors)\n",
    "    X_pred_X, X_pred_XY, error_X, error_XY = calculate_pred_and_errors(\n",
    "        data_test[lag:, 0], \n",
    "        data_X, \n",
    "        data_XY, \n",
    "        best_model_X, \n",
    "        best_model_XY\n",
    "    )\n",
    "    # Plot of X predicted vs prediction error\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    ax[0].plot(X_pred_X, error_X, \"o\")\n",
    "    ax[0].set_xlabel(\"Predicted X values\")\n",
    "    ax[0].set_ylabel(\"Prediction errors\")\n",
    "    ax[0].set_title(\"Model based on X\")\n",
    "    ax[1].plot(X_pred_XY, error_XY, \"o\")\n",
    "    ax[1].set_xlabel(\"Predicted X values\")\n",
    "    ax[1].set_ylabel(\"Prediction errors\")\n",
    "    ax[1].set_title(\"Model based on X and Y\")\n",
    "    plt.suptitle(\"Lag = %d\" % lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706cea49-2e4e-4b80-9dc4-be81936b1cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Test in case of absence of the causality\n",
    "np.random.seed(30)\n",
    "data_noise = np.vstack([x[:-100], np.random.random(10_000)]).T\n",
    "\n",
    "lags = [50, 150]\n",
    "data_noise_train = data_noise[:6000, :]\n",
    "data_noise_val = data_noise[6000:8000, :]\n",
    "data_noise_test = data_noise[8000:, :]\n",
    "\n",
    "results = nlc.nonlincausalityNN(\n",
    "    x=data_noise_train,\n",
    "    maxlag=lags,\n",
    "    NN_config=['d','dr','d','dr'],\n",
    "    NN_neurons=[100,0.05,100,0.05],\n",
    "    x_test=data_noise_test,\n",
    "    run=3,\n",
    "    epochs_num=[50, 50],\n",
    "    learning_rate=[0.001, 0.0001],\n",
    "    batch_size_num=32,\n",
    "    x_val=data_noise_val,\n",
    "    reg_alpha=None,\n",
    "    callbacks=None,\n",
    "    verbose=True,\n",
    "    plot=True,\n",
    ")\n",
    "\n",
    "#%% Example of obtaining the results\n",
    "for lag in lags:\n",
    "    best_model_X_lag50 = results[lag].best_model_X\n",
    "    best_model_XY_lag50 = results[lag].best_model_XY\n",
    "\n",
    "    p_value = results[lag].p_value\n",
    "    test_statistic = results[lag].test_statistic\n",
    "\n",
    "    best_history_X = results[lag].best_history_X\n",
    "    best_history_XY = results[lag].best_history_XY\n",
    "\n",
    "    nlc.plot_history_loss(best_history_X, best_history_XY)\n",
    "    plt.title(\"Lag = %d\" % lag)\n",
    "\n",
    "    best_errors_X = results[lag].best_errors_X\n",
    "    best_errors_XY = results[lag].best_errors_XY\n",
    "\n",
    "    cohens_d = np.abs(\n",
    "        (np.mean(np.abs(best_errors_X)) - np.mean(np.abs(best_errors_XY)))\n",
    "        / np.std([best_errors_X, best_errors_XY])\n",
    "    )\n",
    "    print(\"For lag = %d Cohen's d = %0.3f\" % (lag, cohens_d))\n",
    "    print(f\"test statistic = {test_statistic} p-value = {p_value}\")\n",
    "#%% Example of the measure of the causality change over time\n",
    "\n",
    "data_test_measure = copy.copy(data_test)\n",
    "np.random.seed(30)\n",
    "data_test_measure[:1000, 1] = np.random.random(1000)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(data_test_measure[:, 0], label=\"X\")\n",
    "plt.plot(data_test_measure[:, 1], label=\"Y\")\n",
    "plt.xlabel(\"Number of sample\")\n",
    "plt.ylabel(\"Signals [AU]\")\n",
    "plt.legend()\n",
    "\n",
    "results = nlc.nonlincausalitymeasureNN(\n",
    "    x=data_train,\n",
    "    maxlag=lags,\n",
    "    window=100,\n",
    "    step=1,\n",
    "    NN_config=['d','dr','d','dr'],\n",
    "    NN_neurons=[100,0.05,100,0.05],\n",
    "    x_test=data_test_measure,\n",
    "    run=3,\n",
    "    epochs_num=[50,50],\n",
    "    learning_rate=[0.0001, 0.00001],\n",
    "    batch_size_num=32,\n",
    "    x_val=data_val,\n",
    "    verbose=True,\n",
    "    plot=True,\n",
    ")\n",
    "\n",
    "\n",
    "#%% Example of usage for conditional analysis\n",
    "np.random.seed(30)\n",
    "z = np.random.random([10_000, 2])\n",
    "\n",
    "z_train = z[:6000, :]\n",
    "z_val = z[6000:8000, :]\n",
    "z_test = z[8000:, :]\n",
    "\n",
    "results_conditional = nlc.nonlincausalityNN(\n",
    "    x=data_train,\n",
    "    maxlag=lags,\n",
    "    NN_config=['d','dr','d','dr'],\n",
    "    NN_neurons=[100,0.05,100,0.05],\n",
    "    x_test=data_test,\n",
    "    run=1,\n",
    "    z=z_train,\n",
    "    z_test=z_test,\n",
    "    epochs_num=[50, 50],\n",
    "    learning_rate=[0.0001, 0.00001],\n",
    "    batch_size_num=32,\n",
    "    x_val=data_val,\n",
    "    z_val=z_val,\n",
    "    reg_alpha=None,\n",
    "    callbacks=None,\n",
    "    verbose=True,\n",
    "    plot=True,\n",
    ")\n",
    "# %% Exaple of the usage the package with Scikit-learn model\n",
    "\n",
    "parametres = {\n",
    "    'kernel':['poly', 'rbf'],\n",
    "    'C':[0.01,0.1,1], \n",
    "    'epsilon':[0.01,0.1,1.]\n",
    "}\n",
    "results_skl = nlc.nonlincausality_sklearn(    \n",
    "    x=data_train,\n",
    "    sklearn_model=SVR,\n",
    "    maxlag=lags,\n",
    "    params=parametres,\n",
    "    x_test=data_test,\n",
    "    x_val=data_val,\n",
    "    plot=True)\n",
    "\n",
    "#%% Example of usage other functions for causality analysis\n",
    "\n",
    "# ARIMA/ARIMAX models\n",
    "results_ARIMA = nlc.nonlincausalityARIMA(x=data_train[::10], maxlag=[5,15], x_test=data_test[::10])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "APA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
