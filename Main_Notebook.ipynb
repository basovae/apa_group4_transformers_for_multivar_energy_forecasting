{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c73133af-7360-4762-9048-3aa0b3bb71b4",
   "metadata": {},
   "source": [
    "### Agenda:\n",
    "1. Data Loading & Preprocessing\n",
    "   - Pivoting\n",
    "   - Missing values handling\n",
    "   - Date features creation\n",
    "   - Train/Test split\n",
    "   - Scaling\n",
    "   - Sequences\n",
    "   - Data Loader (incl. indexing for Basisformer)\n",
    "2. Chronos Model\n",
    "3. BasisFormer\n",
    "4. Non-Stationary Transformer\n",
    "5. iTransformer\n",
    "6. Synthtic Data Generation\n",
    "   - Chronos Simulation Framework\n",
    "   - DYNOTEARS Causal Structure\n",
    "   - Non linear causal structure\n",
    "   - Augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "veljByFX557b",
   "metadata": {
    "id": "veljByFX557b"
   },
   "outputs": [],
   "source": [
    "#!pip install torch==2.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370ef93a-0cd3-471b-a6d1-1b3ed3da369c",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1c78b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cad7a09c-a0e5-4eff-b76b-752fcb66665b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f4de819",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.weight_norm as wn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98a448b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import argparse\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c0948b2-e0b7-4862-9a96-8f186c1d4244",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "iGtogtkxzGiT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "iGtogtkxzGiT",
    "outputId": "d63b1548-a355-4ac8-89ed-fc0029c44b31"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>ISO3 Code</th>\n",
       "      <th>Datetime (UTC)</th>\n",
       "      <th>Datetime (Local)</th>\n",
       "      <th>Price (EUR/MWhe)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Austria</td>\n",
       "      <td>AUT</td>\n",
       "      <td>2015-01-01 00:00:00</td>\n",
       "      <td>2015-01-01 01:00:00</td>\n",
       "      <td>17.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Austria</td>\n",
       "      <td>AUT</td>\n",
       "      <td>2015-01-01 01:00:00</td>\n",
       "      <td>2015-01-01 02:00:00</td>\n",
       "      <td>15.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Austria</td>\n",
       "      <td>AUT</td>\n",
       "      <td>2015-01-01 02:00:00</td>\n",
       "      <td>2015-01-01 03:00:00</td>\n",
       "      <td>16.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Austria</td>\n",
       "      <td>AUT</td>\n",
       "      <td>2015-01-01 03:00:00</td>\n",
       "      <td>2015-01-01 04:00:00</td>\n",
       "      <td>17.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Austria</td>\n",
       "      <td>AUT</td>\n",
       "      <td>2015-01-01 04:00:00</td>\n",
       "      <td>2015-01-01 05:00:00</td>\n",
       "      <td>16.38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Country ISO3 Code       Datetime (UTC)     Datetime (Local)  \\\n",
       "0  Austria       AUT  2015-01-01 00:00:00  2015-01-01 01:00:00   \n",
       "1  Austria       AUT  2015-01-01 01:00:00  2015-01-01 02:00:00   \n",
       "2  Austria       AUT  2015-01-01 02:00:00  2015-01-01 03:00:00   \n",
       "3  Austria       AUT  2015-01-01 03:00:00  2015-01-01 04:00:00   \n",
       "4  Austria       AUT  2015-01-01 04:00:00  2015-01-01 05:00:00   \n",
       "\n",
       "   Price (EUR/MWhe)  \n",
       "0             17.93  \n",
       "1             15.17  \n",
       "2             16.38  \n",
       "3             17.38  \n",
       "4             16.38  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##file_path = '/content/all_countries.csv' ## colab path\n",
    "file_path = 'data/all_countries.csv' ## jupyter path\n",
    "df = pd.read_csv(file_path)\n",
    "df = pd.DataFrame(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "vQW9jbwq4lFG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477
    },
    "id": "vQW9jbwq4lFG",
    "outputId": "e9aa57ef-18a2-4243-cd79-51b284512116"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Country</th>\n",
       "      <th>Austria</th>\n",
       "      <th>Belgium</th>\n",
       "      <th>Bulgaria</th>\n",
       "      <th>Croatia</th>\n",
       "      <th>Czechia</th>\n",
       "      <th>Denmark</th>\n",
       "      <th>Estonia</th>\n",
       "      <th>Finland</th>\n",
       "      <th>France</th>\n",
       "      <th>Germany</th>\n",
       "      <th>...</th>\n",
       "      <th>Norway</th>\n",
       "      <th>Poland</th>\n",
       "      <th>Portugal</th>\n",
       "      <th>Romania</th>\n",
       "      <th>Serbia</th>\n",
       "      <th>Slovakia</th>\n",
       "      <th>Slovenia</th>\n",
       "      <th>Spain</th>\n",
       "      <th>Sweden</th>\n",
       "      <th>Switzerland</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Datetime (UTC)</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-01 00:00:00</th>\n",
       "      <td>17.93</td>\n",
       "      <td>34.94</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.20</td>\n",
       "      <td>18.29</td>\n",
       "      <td>23.37</td>\n",
       "      <td>23.37</td>\n",
       "      <td>34.94</td>\n",
       "      <td>17.93</td>\n",
       "      <td>...</td>\n",
       "      <td>27.36</td>\n",
       "      <td>17.18</td>\n",
       "      <td>48.10</td>\n",
       "      <td>44.17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.20</td>\n",
       "      <td>23.25</td>\n",
       "      <td>48.10</td>\n",
       "      <td>23.37</td>\n",
       "      <td>43.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 01:00:00</th>\n",
       "      <td>15.17</td>\n",
       "      <td>32.19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.06</td>\n",
       "      <td>16.04</td>\n",
       "      <td>19.33</td>\n",
       "      <td>19.33</td>\n",
       "      <td>32.19</td>\n",
       "      <td>15.17</td>\n",
       "      <td>...</td>\n",
       "      <td>27.24</td>\n",
       "      <td>17.38</td>\n",
       "      <td>47.33</td>\n",
       "      <td>39.17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.06</td>\n",
       "      <td>22.20</td>\n",
       "      <td>47.33</td>\n",
       "      <td>19.33</td>\n",
       "      <td>38.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 02:00:00</th>\n",
       "      <td>16.38</td>\n",
       "      <td>28.05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.27</td>\n",
       "      <td>14.60</td>\n",
       "      <td>17.66</td>\n",
       "      <td>17.66</td>\n",
       "      <td>23.53</td>\n",
       "      <td>16.38</td>\n",
       "      <td>...</td>\n",
       "      <td>27.16</td>\n",
       "      <td>17.40</td>\n",
       "      <td>42.27</td>\n",
       "      <td>26.93</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.27</td>\n",
       "      <td>19.56</td>\n",
       "      <td>42.27</td>\n",
       "      <td>17.66</td>\n",
       "      <td>35.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 03:00:00</th>\n",
       "      <td>17.38</td>\n",
       "      <td>28.04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.17</td>\n",
       "      <td>14.95</td>\n",
       "      <td>17.53</td>\n",
       "      <td>17.53</td>\n",
       "      <td>22.92</td>\n",
       "      <td>17.38</td>\n",
       "      <td>...</td>\n",
       "      <td>27.15</td>\n",
       "      <td>18.60</td>\n",
       "      <td>38.41</td>\n",
       "      <td>20.94</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.17</td>\n",
       "      <td>18.88</td>\n",
       "      <td>38.41</td>\n",
       "      <td>17.53</td>\n",
       "      <td>30.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 04:00:00</th>\n",
       "      <td>16.38</td>\n",
       "      <td>34.26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.90</td>\n",
       "      <td>14.50</td>\n",
       "      <td>18.07</td>\n",
       "      <td>18.07</td>\n",
       "      <td>34.26</td>\n",
       "      <td>16.38</td>\n",
       "      <td>...</td>\n",
       "      <td>27.30</td>\n",
       "      <td>19.30</td>\n",
       "      <td>35.72</td>\n",
       "      <td>18.52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.90</td>\n",
       "      <td>18.39</td>\n",
       "      <td>35.72</td>\n",
       "      <td>18.07</td>\n",
       "      <td>28.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Country              Austria  Belgium  Bulgaria  Croatia  Czechia  Denmark  \\\n",
       "Datetime (UTC)                                                               \n",
       "2015-01-01 00:00:00    17.93    34.94       NaN      NaN    24.20    18.29   \n",
       "2015-01-01 01:00:00    15.17    32.19       NaN      NaN    22.06    16.04   \n",
       "2015-01-01 02:00:00    16.38    28.05       NaN      NaN    20.27    14.60   \n",
       "2015-01-01 03:00:00    17.38    28.04       NaN      NaN    19.17    14.95   \n",
       "2015-01-01 04:00:00    16.38    34.26       NaN      NaN    17.90    14.50   \n",
       "\n",
       "Country              Estonia  Finland  France  Germany  ...  Norway  Poland  \\\n",
       "Datetime (UTC)                                          ...                   \n",
       "2015-01-01 00:00:00    23.37    23.37   34.94    17.93  ...   27.36   17.18   \n",
       "2015-01-01 01:00:00    19.33    19.33   32.19    15.17  ...   27.24   17.38   \n",
       "2015-01-01 02:00:00    17.66    17.66   23.53    16.38  ...   27.16   17.40   \n",
       "2015-01-01 03:00:00    17.53    17.53   22.92    17.38  ...   27.15   18.60   \n",
       "2015-01-01 04:00:00    18.07    18.07   34.26    16.38  ...   27.30   19.30   \n",
       "\n",
       "Country              Portugal  Romania  Serbia  Slovakia  Slovenia  Spain  \\\n",
       "Datetime (UTC)                                                              \n",
       "2015-01-01 00:00:00     48.10    44.17     NaN     24.20     23.25  48.10   \n",
       "2015-01-01 01:00:00     47.33    39.17     NaN     22.06     22.20  47.33   \n",
       "2015-01-01 02:00:00     42.27    26.93     NaN     20.27     19.56  42.27   \n",
       "2015-01-01 03:00:00     38.41    20.94     NaN     19.17     18.88  38.41   \n",
       "2015-01-01 04:00:00     35.72    18.52     NaN     17.90     18.39  35.72   \n",
       "\n",
       "Country              Sweden  Switzerland  \n",
       "Datetime (UTC)                            \n",
       "2015-01-01 00:00:00   23.37        43.43  \n",
       "2015-01-01 01:00:00   19.33        38.08  \n",
       "2015-01-01 02:00:00   17.66        35.47  \n",
       "2015-01-01 03:00:00   17.53        30.83  \n",
       "2015-01-01 04:00:00   18.07        28.26  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df [['Country','Datetime (UTC)',  'Price (EUR/MWhe)']]\n",
    "df = df.pivot(index='Datetime (UTC)', columns='Country', values='Price (EUR/MWhe)')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "mfQvgSnM3_1v",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mfQvgSnM3_1v",
    "outputId": "9cd1c2f9-c7f7-46d1-df8b-df5c13df26d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country\n",
      "Austria                0\n",
      "Belgium                0\n",
      "Bulgaria           15336\n",
      "Croatia            24096\n",
      "Czechia                0\n",
      "Denmark                0\n",
      "Estonia                0\n",
      "Finland                0\n",
      "France                 0\n",
      "Germany                0\n",
      "Greece                 0\n",
      "Hungary                0\n",
      "Ireland            12480\n",
      "Italy                  0\n",
      "Latvia                 0\n",
      "Lithuania              0\n",
      "Luxembourg             0\n",
      "Netherlands            0\n",
      "North Macedonia    73008\n",
      "Norway                 0\n",
      "Poland                 0\n",
      "Portugal               0\n",
      "Romania                0\n",
      "Serbia             16800\n",
      "Slovakia               0\n",
      "Slovenia               0\n",
      "Spain                  0\n",
      "Sweden                 0\n",
      "Switzerland            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "WObloVqL4aH3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WObloVqL4aH3",
    "outputId": "c1a5702e-1761-4cca-d65c-14966b09d8dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country\n",
      "Austria        0\n",
      "Belgium        0\n",
      "Czechia        0\n",
      "Denmark        0\n",
      "Estonia        0\n",
      "Finland        0\n",
      "France         0\n",
      "Germany        0\n",
      "Greece         0\n",
      "Hungary        0\n",
      "Italy          0\n",
      "Latvia         0\n",
      "Lithuania      0\n",
      "Luxembourg     0\n",
      "Netherlands    0\n",
      "Norway         0\n",
      "Poland         0\n",
      "Portugal       0\n",
      "Romania        0\n",
      "Slovakia       0\n",
      "Slovenia       0\n",
      "Spain          0\n",
      "Sweden         0\n",
      "Switzerland    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna(axis=1)\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "AB6vyJX-C6h3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AB6vyJX-C6h3",
    "outputId": "aa9990f8-cb44-4367-b4ff-e2a1bf1732d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Datetime (UTC)  Austria  Belgium  Czechia  Denmark  Estonia  Finland  \\\n",
      "0  2015-01-01 00:00:00    17.93    34.94    24.20    18.29    23.37    23.37   \n",
      "1  2015-01-01 01:00:00    15.17    32.19    22.06    16.04    19.33    19.33   \n",
      "2  2015-01-01 02:00:00    16.38    28.05    20.27    14.60    17.66    17.66   \n",
      "3  2015-01-01 03:00:00    17.38    28.04    19.17    14.95    17.53    17.53   \n",
      "4  2015-01-01 04:00:00    16.38    34.26    17.90    14.50    18.07    18.07   \n",
      "\n",
      "   France  Germany  Greece  ...  Netherlands  Norway  Poland  Portugal  \\\n",
      "0   34.94    17.93   48.78  ...        34.94   27.36   17.18     48.10   \n",
      "1   32.19    15.17   31.10  ...        32.19   27.24   17.38     47.33   \n",
      "2   23.53    16.38   20.78  ...        28.05   27.16   17.40     42.27   \n",
      "3   22.92    17.38   25.40  ...        28.04   27.15   18.60     38.41   \n",
      "4   34.26    16.38   26.00  ...        34.26   27.30   19.30     35.72   \n",
      "\n",
      "   Romania  Slovakia  Slovenia  Spain  Sweden  Switzerland  \n",
      "0    44.17     24.20     23.25  48.10   23.37        43.43  \n",
      "1    39.17     22.06     22.20  47.33   19.33        38.08  \n",
      "2    26.93     20.27     19.56  42.27   17.66        35.47  \n",
      "3    20.94     19.17     18.88  38.41   17.53        30.83  \n",
      "4    18.52     17.90     18.39  35.72   18.07        28.26  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "df.reset_index(inplace=True)\n",
    "df.columns.name = None\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99211c4c-9b8f-4d60-9bb6-add80102ca5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last time point available: 2024-04-30 23:00:00\n"
     ]
    }
   ],
   "source": [
    "df['Datetime (UTC)'] = pd.to_datetime(df['Datetime (UTC)'])\n",
    "last_time_point = df['Datetime (UTC)'].max()\n",
    "print(\"Last time point available:\", last_time_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9365312-6296-4b97-ace6-9dd9191fccea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Datetime (UTC)  Austria  Belgium  Czechia  Denmark  Estonia  \\\n",
      "79633 2024-02-01 01:00:00    66.59    44.47    80.01    30.49    26.71   \n",
      "79634 2024-02-01 02:00:00    64.13    45.15    80.01    31.12    38.22   \n",
      "79635 2024-02-01 03:00:00    69.80    49.02    80.01    31.95    27.71   \n",
      "79636 2024-02-01 04:00:00    80.78    57.56    87.10    35.01    20.10   \n",
      "79637 2024-02-01 05:00:00    86.95    70.56    95.00    39.04    72.59   \n",
      "...                   ...      ...      ...      ...      ...      ...   \n",
      "81787 2024-04-30 19:00:00    87.82    87.02    88.29    66.05    88.36   \n",
      "81788 2024-04-30 20:00:00    77.50    75.39    77.99    54.09    78.02   \n",
      "81789 2024-04-30 21:00:00    76.10    77.51    75.93    50.98    54.93   \n",
      "81790 2024-04-30 22:00:00    64.22    58.00    67.16    44.71    38.00   \n",
      "81791 2024-04-30 23:00:00    54.89    54.28    56.35    44.18    36.61   \n",
      "\n",
      "       Finland  France  Germany  Greece  ...  Netherlands  Norway  Poland  \\\n",
      "79633    -2.49   46.66    42.16   80.00  ...        43.04   34.35   42.38   \n",
      "79634    -2.50   47.04    43.05   73.06  ...        43.94   34.62   40.93   \n",
      "79635    -2.07   51.06    46.91   83.80  ...        47.63   37.25   40.89   \n",
      "79636    -1.84   59.82    55.34   99.57  ...        56.06   39.16   51.65   \n",
      "79637    -0.10   72.42    69.01  107.36  ...        69.20   40.32   72.59   \n",
      "...        ...     ...      ...     ...  ...          ...     ...     ...   \n",
      "81787    88.36   83.91    88.37   80.01  ...        88.15   53.42   88.36   \n",
      "81788    78.02   70.98    77.97   77.53  ...        80.00   52.52   83.29   \n",
      "81789    54.93   75.79    75.69   75.97  ...        81.80   50.05   80.97   \n",
      "81790    38.00   35.01    66.81   36.99  ...        76.77   43.80   67.43   \n",
      "81791    36.61   34.99    55.09   34.86  ...        79.89   43.30   74.02   \n",
      "\n",
      "       Portugal  Romania  Slovakia  Slovenia  Spain  Sweden  Switzerland  \n",
      "79633     55.10    80.00     91.00     71.73  55.10   -2.49        70.80  \n",
      "79634     54.60    73.06     80.19     67.66  54.60   -2.50        73.85  \n",
      "79635     53.47    83.80     96.02     75.10  53.47   -2.07        74.06  \n",
      "79636     59.82    99.57    116.13     87.74  59.82   -1.84        74.09  \n",
      "79637     72.42   107.36    108.87     95.63  72.42   -0.10        86.00  \n",
      "...         ...      ...       ...       ...    ...     ...          ...  \n",
      "81787     83.91    88.08     88.15     88.09  83.91   55.47        88.81  \n",
      "81788     70.98    77.53     77.74     77.19  70.98   52.77        78.03  \n",
      "81789     65.00    75.97     75.94     76.02  65.00   50.05        71.07  \n",
      "81790     35.01    64.74     65.81     62.97  35.01   38.00        57.37  \n",
      "81791     34.99    54.86     55.49     53.86  34.99   36.61        53.04  \n",
      "\n",
      "[2159 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "start_date = '2024-02-01 01:00:00' ## FILTERING ONLY FOR 3 MONTHS\n",
    "df = df[df['Datetime (UTC)'] >= pd.to_datetime(start_date)]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "_Jtyfje24Pdq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Jtyfje24Pdq",
    "outputId": "507d28c6-0347-4446-9078-c6fda9ffa9a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Datetime (UTC)  Austria  Belgium  Czechia  Denmark  Estonia  \\\n",
      "79633 2024-02-01 01:00:00    66.59    44.47    80.01    30.49    26.71   \n",
      "79634 2024-02-01 02:00:00    64.13    45.15    80.01    31.12    38.22   \n",
      "79635 2024-02-01 03:00:00    69.80    49.02    80.01    31.95    27.71   \n",
      "79636 2024-02-01 04:00:00    80.78    57.56    87.10    35.01    20.10   \n",
      "79637 2024-02-01 05:00:00    86.95    70.56    95.00    39.04    72.59   \n",
      "\n",
      "       Finland  France  Germany  Greece  ...  Romania  Slovakia  Slovenia  \\\n",
      "79633    -2.49   46.66    42.16   80.00  ...    80.00     91.00     71.73   \n",
      "79634    -2.50   47.04    43.05   73.06  ...    73.06     80.19     67.66   \n",
      "79635    -2.07   51.06    46.91   83.80  ...    83.80     96.02     75.10   \n",
      "79636    -1.84   59.82    55.34   99.57  ...    99.57    116.13     87.74   \n",
      "79637    -0.10   72.42    69.01  107.36  ...   107.36    108.87     95.63   \n",
      "\n",
      "       Spain  Sweden  Switzerland  month  day  weekday  hour  \n",
      "79633  55.10   -2.49        70.80      2    1        3     1  \n",
      "79634  54.60   -2.50        73.85      2    1        3     2  \n",
      "79635  53.47   -2.07        74.06      2    1        3     3  \n",
      "79636  59.82   -1.84        74.09      2    1        3     4  \n",
      "79637  72.42   -0.10        86.00      2    1        3     5  \n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "df['month'] = df['Datetime (UTC)'].apply(lambda row: row.month)\n",
    "df['day'] = df['Datetime (UTC)'].apply(lambda row: row.day)\n",
    "df['weekday'] = df['Datetime (UTC)'].apply(lambda row: row.weekday())\n",
    "df['hour'] = df['Datetime (UTC)'].apply(lambda row: row.hour)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80mxqDAh5Mwd",
   "metadata": {
    "id": "80mxqDAh5Mwd"
   },
   "outputs": [],
   "source": [
    "# separating the electricity prices and timestamp features\n",
    "electricity_prices_df = df[['Datetime (UTC)', 'Austria', 'Belgium', 'Czechia', 'Denmark', 'Estonia', 'Finland', 'France',\n",
    "              'Germany', 'Greece', 'Hungary', 'Italy', 'Latvia', 'Lithuania', 'Luxembourg',\n",
    "             'Netherlands', 'Norway', 'Poland', 'Portugal', 'Romania', 'Slovakia',\n",
    "             'Slovenia', 'Spain', 'Sweden', 'Switzerland']]\n",
    "timestamp_features_df = df[['Datetime (UTC)', 'month', 'day', 'weekday', 'hour']]\n",
    "\n",
    "# defining the split ratio\n",
    "train_size = 0.8\n",
    "train_size_electricity = int(len(electricity_prices_df) * train_size)\n",
    "train_size_timestamp = int(len(timestamp_features_df) * train_size)\n",
    "\n",
    "# spliting the data into train and test sets\n",
    "electricity_prices_train = electricity_prices_df[:train_size_electricity]\n",
    "electricity_prices_test = electricity_prices_df[train_size_electricity:]\n",
    "timestamp_features_train = timestamp_features_df[:train_size_timestamp]\n",
    "timestamp_features_test = timestamp_features_df[train_size_timestamp:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95180c89-5259-40f8-af8b-e90241aa8864",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(electricity_prices_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a2c3f05-460c-4d80-868e-48ff8f5cfd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#country_names = electricity_prices_train.drop(columns=['Datetime (UTC)']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4oRyT6un5WWm",
   "metadata": {
    "id": "4oRyT6un5WWm"
   },
   "outputs": [],
   "source": [
    "# rescaling the electricity prices\n",
    "scaler = StandardScaler()\n",
    "\n",
    "electricity_prices_train_scaled = scaler.fit_transform(electricity_prices_train.drop(columns=['Datetime (UTC)']))\n",
    "electricity_prices_test_scaled = scaler.transform(electricity_prices_test.drop(columns=['Datetime (UTC)']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07d72a8e-4df2-4255-bdb3-3353426389cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#electricity_prices_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "407a011b-3187-4e94-9c3a-e36c17253639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, seq_length, pred_length, label_length, curr_model):\n",
    "    seq_x = [] # storing for input seqiences\n",
    "    seq_y = [] # storing for output seqiences\n",
    "    for i in range(len(data) - seq_length - pred_length):\n",
    "        seq_x.append(data[i:i+seq_length])\n",
    "        if curr_model in [\"basis_former\", \"itransformer\", \"ns_autoformer\"]:\n",
    "          seq_y.append(data[i+seq_length-label_length:i+seq_length+pred_length])\n",
    "        else: ## only chronos\n",
    "          seq_y.append(data[i+seq_length:i+seq_length+pred_length])\n",
    "    return np.array(seq_x), np.array(seq_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c17fd64-77c4-42ab-9e63-bf919d801d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(seq_x, seq_y, seq_x_mark, seq_y_mark, batch_size, curr_model):\n",
    "    seq_x = torch.tensor(seq_x, dtype=torch.float32)\n",
    "    seq_y = torch.tensor(seq_y, dtype=torch.float32)\n",
    "    seq_x_mark = torch.tensor(seq_x_mark, dtype=torch.float32)\n",
    "    seq_y_mark = torch.tensor(seq_y_mark, dtype=torch.float32)\n",
    "    \n",
    "    if curr_model == \"basis_former\":\n",
    "        indices = []\n",
    "        total_len = len(seq_x)\n",
    "        for i in range(total_len):\n",
    "            index_list = np.arange(i, i + len(seq_x[0]) + len(seq_y[0]), 1)\n",
    "            norm_index = index_list / total_len\n",
    "            indices.append(norm_index)\n",
    "        indices = torch.tensor(indices, dtype=torch.float32)\n",
    "        dataset = TensorDataset(seq_x, seq_y, seq_x_mark, seq_y_mark, indices)\n",
    "    else:\n",
    "        dataset = TensorDataset(seq_x, seq_y, seq_x_mark, seq_y_mark)\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=2, shuffle=True, drop_last=True)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ded9c3f-9b9c-48cc-8130-762d94b62f32",
   "metadata": {},
   "source": [
    "# Chronos\n",
    "\n",
    "zero shot evaluation with Chronos Pretrained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cca768-c6f9-4d30-ad48-32c86e9ae2c1",
   "metadata": {},
   "source": [
    "# Chronos 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ef7efd7-9214-4d25-bca0-afe8d19cad4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, seq_length, pred_length, label_length, curr_model):\n",
    "    seq_x = []  # storing for input sequences\n",
    "    seq_y = []  # storing for output sequences\n",
    "    for i in range(len(data) - seq_length - pred_length):\n",
    "        seq_x.append(data[i:i+seq_length])\n",
    "        if curr_model in [\"basis_former\", \"itransformer\", \"ns_autoformer\"]:\n",
    "            seq_y.append(data[i+seq_length-label_length:i+seq_length+pred_length])\n",
    "        elif curr_model == \"chronos\":\n",
    "            # For Chronos, we only need the input sequence\n",
    "            seq_y.append(data[i+seq_length])  # Single step forecast\n",
    "        else:\n",
    "            seq_y.append(data[i+seq_length:i+seq_length+pred_length])\n",
    "    return np.array(seq_x), np.array(seq_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8fa73ec-8a05-4565-b335-0886a508f267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(seq_x, seq_y, seq_x_mark, seq_y_mark, batch_size, curr_model):\n",
    "    seq_x = torch.tensor(seq_x, dtype=torch.float32)\n",
    "    seq_y = torch.tensor(seq_y, dtype=torch.float32)\n",
    "    seq_x_mark = torch.tensor(seq_x_mark, dtype=torch.float32)\n",
    "    seq_y_mark = torch.tensor(seq_y_mark, dtype=torch.float32)\n",
    "    \n",
    "    if curr_model == \"basis_former\":\n",
    "        indices = []\n",
    "        total_len = len(seq_x)\n",
    "        for i in range(total_len):\n",
    "            index_list = np.arange(i, i + len(seq_x[0]) + len(seq_y[0]), 1)\n",
    "            norm_index = index_list / total_len\n",
    "            indices.append(norm_index)\n",
    "        indices = torch.tensor(indices, dtype=torch.float32)\n",
    "        dataset = TensorDataset(seq_x, seq_y, seq_x_mark, seq_y_mark, indices)\n",
    "    elif curr_model == \"chronos\":\n",
    "        # For Chronos, we only need the input sequence (seq_x)\n",
    "        dataset = TensorDataset(seq_x.reshape(seq_x.shape[0], -1))  # Flatten the input sequence\n",
    "    else:\n",
    "        dataset = TensorDataset(seq_x, seq_y, seq_x_mark, seq_y_mark)\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=2, shuffle=True, drop_last=True)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9d4e2e76-e80a-45da-aba7-c82e5dae03b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ekaterinabasova/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 8.33 GB, other allocations: 4.66 MB, max allowed: 9.07 GB). Tried to allocate 812.50 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 42\u001b[0m\n\u001b[1;32m     38\u001b[0m chronos_input \u001b[38;5;241m=\u001b[39m prepare_chronos_data(dataloader)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Now you can use chronos_input with your Chronos pipeline\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Assuming you have a Chronos pipeline already set up\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m forecast \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchronos_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Store the forecast results\u001b[39;00m\n\u001b[1;32m     49\u001b[0m results[country] \u001b[38;5;241m=\u001b[39m forecast\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/chronos/chronos.py:512\u001b[0m, in \u001b[0;36mChronosPipeline.predict\u001b[0;34m(self, context, prediction_length, num_samples, temperature, top_k, top_p, limit_prediction_length)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    509\u001b[0m     token_ids, attention_mask, scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mcontext_input_transform(\n\u001b[1;32m    510\u001b[0m         context_tensor\n\u001b[1;32m    511\u001b[0m     )\n\u001b[0;32m--> 512\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_length\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39moutput_transform(\n\u001b[1;32m    522\u001b[0m         samples\u001b[38;5;241m.\u001b[39mto(scale\u001b[38;5;241m.\u001b[39mdevice), scale\n\u001b[1;32m    523\u001b[0m     )\n\u001b[1;32m    525\u001b[0m     predictions\u001b[38;5;241m.\u001b[39mappend(prediction)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/chronos/chronos.py:336\u001b[0m, in \u001b[0;36mChronosModel.forward\u001b[0;34m(self, input_ids, attention_mask, prediction_length, num_samples, temperature, top_k, top_p)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m top_p \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    334\u001b[0m     top_p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtop_p\n\u001b[0;32m--> 336\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGenerationConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprediction_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprediction_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmodel_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq2seq\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    353\u001b[0m     preds \u001b[38;5;241m=\u001b[39m preds[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# remove the decoder start token\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/generation/utils.py:1758\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1750\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1751\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1752\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1753\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1754\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1755\u001b[0m     )\n\u001b[1;32m   1757\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 1758\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1759\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1760\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1761\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1762\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1763\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1764\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1765\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1766\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1767\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1769\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   1770\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1771\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1772\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config) \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1773\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/generation/utils.py:2397\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2394\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2396\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2397\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2398\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2400\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2401\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2402\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2405\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:1740\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1737\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m-> 1740\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1749\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1753\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1755\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:1107\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1092\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1093\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39mforward,\n\u001b[1;32m   1094\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1104\u001b[0m         output_attentions,\n\u001b[1;32m   1105\u001b[0m     )\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1107\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:717\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    715\u001b[0m     query_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 717\u001b[0m cross_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    728\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    730\u001b[0m \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:628\u001b[0m, in \u001b[0;36mT5LayerCrossAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, query_length, output_attentions)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    617\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    625\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    626\u001b[0m ):\n\u001b[1;32m    627\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 628\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEncDecAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormed_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_value_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    639\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    640\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:524\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    519\u001b[0m value_states \u001b[38;5;241m=\u001b[39m project(\n\u001b[1;32m    520\u001b[0m     hidden_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv, key_value_states, past_key_value[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    521\u001b[0m )\n\u001b[1;32m    523\u001b[0m \u001b[38;5;66;03m# compute scores\u001b[39;00m\n\u001b[0;32m--> 524\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\u001b[39;00m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m position_bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_relative_attention_bias:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 8.33 GB, other allocations: 4.66 MB, max allowed: 9.07 GB). Tried to allocate 812.50 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "# Function to prepare data for Chronos\n",
    "def prepare_chronos_data(dataloader):\n",
    "    chronos_data = []\n",
    "    for batch in dataloader:\n",
    "        chronos_data.append(batch[0])  # Only take the first element (seq_x)\n",
    "    return torch.cat(chronos_data, dim=0)\n",
    "\n",
    "pipeline = ChronosPipeline.from_pretrained(\n",
    "    \"amazon/chronos-t5-small\",\n",
    "    device_map=\"mps\",  # use \"cpu\" for CPU inference and \"mps\" for Apple Silicon\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "seq_length = 24\n",
    "pred_length = 12\n",
    "label_length = 12\n",
    "batch_size = 32\n",
    "countries = electricity_prices_train.columns.drop('Datetime (UTC)').tolist()\n",
    "seq_x_mark = timestamp_features_df.iloc[:len(electricity_prices_train)].drop(columns=['Datetime (UTC)']).values\n",
    "seq_y_mark = timestamp_features_df.iloc[len(electricity_prices_train):].drop(columns=['Datetime (UTC)']).values\n",
    "\n",
    "\n",
    "# Assume you have your data prepared as before\n",
    "# data, seq_x_mark, seq_y_mark = ...\n",
    "\n",
    "results = {}\n",
    "\n",
    "for country in countries:\n",
    "    # Extract data for the current country\n",
    "    country_data = electricity_prices_train_scaled[:, electricity_prices_train.columns.get_loc(country)]\n",
    "    \n",
    "    seq_x, seq_y = create_sequences(country_data, seq_length, pred_length, label_length, \"chronos\")\n",
    "    dataloader = create_dataloader(seq_x, seq_y, seq_x_mark, seq_y_mark, batch_size, \"chronos\")\n",
    "    \n",
    "    # Prepare data for Chronos\n",
    "    chronos_input = prepare_chronos_data(dataloader)\n",
    "    \n",
    "    # Now you can use chronos_input with your Chronos pipeline\n",
    "    # Assuming you have a Chronos pipeline already set up\n",
    "    forecast = pipeline.predict(\n",
    "        context=chronos_input,\n",
    "        prediction_length=pred_length,\n",
    "        num_samples=20,\n",
    "    )\n",
    "    \n",
    "    # Store the forecast results\n",
    "    results[country] = forecast\n",
    "    \n",
    "    print(f\"Completed forecast for {country}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c3f273-da3a-4f78-9b03-332f66114916",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "def create_country_tensors(data: np.ndarray, timestamps: pd.DataFrame, sequence_length: int) -> Dict[str, List[torch.Tensor]]:\n",
    "    country_tensors = {}\n",
    "    \n",
    "    for i in range(data.shape[1]):  # Iterate through each country column\n",
    "        country_data = data[:, i]\n",
    "        \n",
    "        # Create sequences\n",
    "        X = []\n",
    "        for j in range(len(country_data) - sequence_length):\n",
    "            X.append(country_data[j:j+sequence_length])\n",
    "        \n",
    "        # Convert to tensor\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        \n",
    "        # Create timestamp tensor\n",
    "        timestamp_tensor = torch.tensor(timestamps.iloc[sequence_length:].values, dtype=torch.float32)\n",
    "        \n",
    "        country_tensors[f\"Country_{i}\"] = [X_tensor, timestamp_tensor]\n",
    "    \n",
    "    return country_tensors\n",
    "\n",
    "# Define sequence length\n",
    "sequence_length = 24  # Adjust as needed\n",
    "\n",
    "# Create tensors for training data\n",
    "train_tensors = create_country_tensors(\n",
    "    electricity_prices_train_scaled, \n",
    "    timestamp_features_df.iloc[:train_size_timestamp], \n",
    "    sequence_length\n",
    ")\n",
    "\n",
    "# Create tensors for test data\n",
    "test_tensors = create_country_tensors(\n",
    "    electricity_prices_test_scaled, \n",
    "    timestamp_features_df.iloc[train_size_timestamp:], \n",
    "    sequence_length\n",
    ")\n",
    "\n",
    "# Function to predict using Chronos for a single country\n",
    "def predict_chronos(pipeline, country_data, prediction_length, num_samples):\n",
    "    forecast = pipeline.predict(\n",
    "        context=country_data[0],  # Use the price data tensor\n",
    "        prediction_length=prediction_length,\n",
    "        num_samples=num_samples,\n",
    "    )\n",
    "    return forecast\n",
    "\n",
    "# Loop through each country and make predictions\n",
    "results = {}\n",
    "for country, tensors in train_tensors.items():\n",
    "    # Assuming you have a Chronos pipeline already set up\n",
    "    forecast = predict_chronos(pipeline, tensors, prediction_length=12, num_samples=20)\n",
    "    results[country] = forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2617ec8b-f710-450b-81b1-1f8ce6f4fba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/amazon-science/chronos-forecasting.git\n",
    "import chronos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da473f90-664c-449e-9396-dc9d8adf3ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "seq_length = 48 # one week\n",
    "pred_length = 48 # two days ahead\n",
    "label_length = 48\n",
    "curr_model = \"chronos\"\n",
    "\n",
    "train_seq_x, train_seq_y = create_sequences(electricity_prices_train_scaled, seq_length, pred_length, label_length, curr_model)\n",
    "train_seq_x_mark, train_seq_y_mark = create_sequences(timestamp_features_train.drop(columns=['Datetime (UTC)']).values, seq_length, pred_length, label_length, curr_model)\n",
    "\n",
    "\n",
    "test_seq_x, test_seq_y = create_sequences(electricity_prices_test_scaled, seq_length, pred_length,label_length, curr_model)\n",
    "test_seq_x_mark, test_seq_y_mark = create_sequences(timestamp_features_test.drop(columns=['Datetime (UTC)']).values, seq_length, pred_length, label_length, curr_model)\n",
    "\n",
    "\n",
    "# converting sequences to PyTorch DataLoader objects\n",
    "train_loader = create_dataloader(train_seq_x, train_seq_y, train_seq_x_mark, train_seq_y_mark, batch_size, curr_model)\n",
    "test_loader = create_dataloader(test_seq_x, test_seq_y, test_seq_x_mark, test_seq_y_mark, batch_size, curr_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99270086-7efc-4968-9720-98413902bdc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample testing sequence x: [[-1.47514259 -1.49006706 -1.56686779 ... -0.29106655 -1.84177497\n",
      "  -0.90996892]\n",
      " [-1.67138901 -1.71409752 -1.76082611 ... -0.49449098 -1.76060728\n",
      "  -1.31408237]\n",
      " [-1.66016387 -1.79509619 -1.74650184 ... -0.60904395 -1.71469425\n",
      "  -1.39936291]\n",
      " ...\n",
      " [ 0.65492464 -0.15813289  0.81173485 ... -0.29613526 -1.88891783\n",
      "   0.61635889]\n",
      " [ 0.53802835 -0.37781708  0.56860945 ... -0.62391218 -1.953688\n",
      "   0.39492022]\n",
      " [ 0.5388025  -0.51887329  0.50318239 ... -0.76076749 -1.95204825\n",
      "   0.20836905]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample testing sequence x:\", test_seq_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22e68564-a944-4a6f-b55d-9230a32c9a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch shape: torch.Size([8, 48, 24])\n",
      "Output batch shape: torch.Size([8, 48, 24])\n",
      "Input mark shape: torch.Size([8, 48, 4])\n",
      "Output mark shape: torch.Size([8, 48, 4])\n"
     ]
    }
   ],
   "source": [
    "# Check the dimensions\n",
    "for batch in test_loader:\n",
    "    batch_x, batch_y, batch_x_mark, batch_y_mark = batch\n",
    "    print(f'Input batch shape: {batch_x.shape}')\n",
    "    print(f'Output batch shape: {batch_y.shape}')\n",
    "    print(f'Input mark shape: {batch_x_mark.shape}')\n",
    "    print(f'Output mark shape: {batch_y_mark.shape}')\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0297963a-162b-417c-9d36-eff09e474633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Get forecasts for the given time series.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        context\n",
      "            Input series. This is either a 1D tensor, or a list\n",
      "            of 1D tensors, or a 2D tensor whose first dimension\n",
      "            is batch. In the latter case, use left-padding with\n",
      "            ``torch.nan`` to align series of different lengths.\n",
      "        prediction_length\n",
      "            Time steps to predict. Defaults to what specified\n",
      "            in ``self.model.config``.\n",
      "        num_samples\n",
      "            Number of sample paths to predict. Defaults to what\n",
      "            specified in ``self.model.config``.\n",
      "        temperature\n",
      "            Temperature to use for generating sample tokens.\n",
      "            Defaults to what specified in ``self.model.config``.\n",
      "        top_k\n",
      "            Top-k parameter to use for generating sample tokens.\n",
      "            Defaults to what specified in ``self.model.config``.\n",
      "        top_p\n",
      "            Top-p parameter to use for generating sample tokens.\n",
      "            Defaults to what specified in ``self.model.config``.\n",
      "        limit_prediction_length\n",
      "            Force prediction length smaller or equal than the\n",
      "            built-in prediction length from the model. True by\n",
      "            default. When true, fail loudly if longer predictions\n",
      "            are requested, otherwise longer predictions are allowed.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        samples\n",
      "            Tensor of sample forecasts, of shape\n",
      "            (batch_size, num_samples, prediction_length).\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "from chronos import ChronosPipeline\n",
    "print(ChronosPipeline.predict.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7a7e22-51e5-4c3c-b3f2-d51498587192",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test_loader:\n",
    "    print(batch)\n",
    "    break  # Print only the first batch and break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bd82e9-d24f-4a39-9a5c-77821d6b2667",
   "metadata": {},
   "source": [
    "1. Pad Sequences to the Same Length: Ensure all sequences within a batch have the same length.\n",
    "2. Flatten the Padded Sequences: Convert the 3D tensor [batch_size, seq_length, num_features] to a 2D tensor [total_sequences, seq_length * num_features]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4fd1366b-cc21-4f51-8478-daa1b37c0ebc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ChronosPipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[0;32m----> 4\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mChronosPipeline\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamazon/chronos-t5-small\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16,  \u001b[38;5;66;03m# Change to torch.float16 if necessary\u001b[39;00m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# padding sequences within a batch\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpad_sequence\u001b[39m(sequences, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding_value\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mnan):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ChronosPipeline' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "pipeline = ChronosPipeline.from_pretrained(\n",
    "    \"amazon/chronos-t5-small\",\n",
    "    device_map=\"cpu\",\n",
    "    torch_dtype=torch.bfloat16,  # Change to torch.float16 if necessary\n",
    ")\n",
    "\n",
    "\n",
    "# padding sequences within a batch\n",
    "def pad_sequence(sequences, batch_first=True, padding_value=torch.nan):\n",
    "    return torch.nn.utils.rnn.pad_sequence(sequences, batch_first=batch_first, padding_value=padding_value)\n",
    "\n",
    "def pad_batch(batch):\n",
    "    input_ids = batch[0]\n",
    "    sequences = [input_ids[i] for i in range(input_ids.size(0))]\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True)\n",
    "    return padded_sequences\n",
    "\n",
    "context = []\n",
    "\n",
    "# Iterate over the DataLoader to extract and pad input_ids\n",
    "for batch in test_loader:\n",
    "    input_ids_padded = pad_batch(batch)\n",
    "    context.append(input_ids_padded)\n",
    "\n",
    "# Concatenate the padded tensors to form a 3D tensor\n",
    "context_tensor_3d = torch.cat(context, dim=0)  # Concatenate along the batch dimension\n",
    "\n",
    "# Flatten the 3D tensor to 2D\n",
    "batch_size, seq_length, num_features = context_tensor_3d.shape\n",
    "context_tensor_2d = context_tensor_3d.view(batch_size, -1)\n",
    "\n",
    "# Now you can use the context tensor with your pipeline\n",
    "print(context_tensor_2d.shape)  # To verify the shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fb6121-1efc-4234-91af-39611924149f",
   "metadata": {},
   "source": [
    "This transformation suggests that each batch of size 24 (sequences) with 96 time steps and 24 features per time step has been flattened into a single 2D tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47829b56-b79b-4efe-9eba-2bfbbad6fea6",
   "metadata": {},
   "source": [
    "Original Shape:\n",
    "Batch size: 24\n",
    "Sequence length: 96\n",
    "Number of features: 24\n",
    "\n",
    "Flattened Shape Calculation:\n",
    "Flattened feature size: 96Ã—24=2304\n",
    "Total number of sequences (batches) after concatenation: 16200\n",
    "\n",
    "\n",
    "The pipeline will process each of these 16,200 sequences, each with 2,304 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632d0df2-a154-46ec-9987-8bba8f396338",
   "metadata": {},
   "outputs": [],
   "source": [
    "del context\n",
    "del context_tensor_3d\n",
    "\n",
    "# predictions\n",
    "# 1 hour of running for 2 months\n",
    "forecast = pipeline.predict(\n",
    "    context=context_tensor_2d,\n",
    "    prediction_length=48,\n",
    "    num_samples=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bba3dd-02a1-4d90-a50b-e11362b3d12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f90544-308f-40b0-a176-45d74c0ba5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the forecast to a numpy array\n",
    "forecast_np = forecast[0].numpy()\n",
    "\n",
    "# Check the shape of forecast_np\n",
    "print(forecast_np.shape)  # This should print (num_samples, prediction_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6c5993-5e40-485a-888d-331dd33b7e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_reshaped = forecast_np.flatten()  # Flatten to (48,)\n",
    "forecast_reshaped = forecast_reshaped.reshape(-1, 1)  # Reshape to (48, 1)\n",
    "print(forecast_reshaped.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ecf9cb-e4f4-4468-9818-c8c6fbf950c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaled_forecast = (forecast_reshaped * scaler.scale_[0]) + scaler.mean_[0]\n",
    "rescaled_forecast = rescaled_forecast.reshape(forecast_np.shape)\n",
    "print(rescaled_forecast.shape)  # Should print (1, 48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63fa5d6-0f1e-4c29-924c-aa327b8351e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_index = range(len(electricity_prices_test_scaled), len(electricity_prices_test_scaled) + 48)\n",
    "\n",
    "# Compute quantiles for the forecast\n",
    "low, median, high = np.quantile(forecast, [0.1, 0.5, 0.9], axis=0)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(range(len(electricity_prices_test_scaled)), electricity_prices_test_scaled, color=\"royalblue\", label=\"historical data\")\n",
    "plt.plot(forecast_index, median, color=\"tomato\", label=\"median forecast\")\n",
    "plt.fill_between(forecast_index, low, high, color=\"tomato\", alpha=0.3, label=\"80% prediction interval\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Electricity Prices')\n",
    "plt.title('Electricity Prices Forecast')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ee0fbc-fd5f-4b8e-99f0-e0d05ff0826d",
   "metadata": {},
   "source": [
    "# Basisformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df01bc9-9596-49f6-ace9-1acf18f1c23e",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ILk9LX4k5Yuu",
   "metadata": {
    "id": "ILk9LX4k5Yuu"
   },
   "outputs": [],
   "source": [
    "seq_length = 96\n",
    "pred_length = 48\n",
    "label_length = 48\n",
    "curr_model = \"basis_former\"\n",
    "\n",
    "train_seq_x, train_seq_y = create_sequences(electricity_prices_train_scaled, seq_length, pred_length, label_length, curr_model)\n",
    "train_seq_x_mark, train_seq_y_mark = create_sequences(timestamp_features_train.drop(columns=['Datetime (UTC)']).values, seq_length, pred_length, label_length, curr_model)\n",
    "\n",
    "\n",
    "test_seq_x, test_seq_y = create_sequences(electricity_prices_test_scaled, seq_length, pred_length,label_length, curr_model)\n",
    "test_seq_x_mark, test_seq_y_mark = create_sequences(timestamp_features_test.drop(columns=['Datetime (UTC)']).values, seq_length, pred_length, label_length, curr_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IeE-jvzOEVw-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IeE-jvzOEVw-",
    "outputId": "d4778504-79d4-475b-c868-17b3994fd3fc"
   },
   "outputs": [],
   "source": [
    "print(\"Sample training sequence x:\", train_seq_x[0])\n",
    "print(\"Sample training sequence y:\", train_seq_y[0])\n",
    "print(\"Sample training sequence x mark:\", train_seq_x_mark[0])\n",
    "print(\"Sample training sequence y mark:\", train_seq_y_mark[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OrL7qSyl5bqB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OrL7qSyl5bqB",
    "outputId": "01496ef4-4d84-4630-dc4b-5a1b9c36a404"
   },
   "outputs": [],
   "source": [
    "batch_size = 24\n",
    "\n",
    "# converting sequences to PyTorch DataLoader objects\n",
    "train_loader = create_dataloader(train_seq_x, train_seq_y, train_seq_x_mark, train_seq_y_mark, batch_size, curr_model)\n",
    "test_loader = create_dataloader(test_seq_x, test_seq_y, test_seq_x_mark, test_seq_y_mark, batch_size, curr_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca8679b-4ac1-42ab-8065-f6f2519e40cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    print(f\"Batch contains {len(batch)} items:\")\n",
    "    for i, item in enumerate(batch):\n",
    "        print(f\"Item {i}: Shape = {item.shape if torch.is_tensor(item) else 'Not a tensor'}\")\n",
    "    break  # Just print the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Des1ClFuevfI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "Des1ClFuevfI",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "538a4886-5959-4a8c-a4e8-3239df3ed5ec"
   },
   "outputs": [],
   "source": [
    "##pip install adabelief_pytorch==0.2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c7af02-3c5d-4a35-a1e1-ed6266bed132",
   "metadata": {},
   "source": [
    "## Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f18f708-9681-4f6a-b70b-0ad9009722f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import Basisformer.model\n",
    "importlib.reload(Basisformer.model)\n",
    "from Basisformer.model import Basisformer\n",
    "\n",
    "import Basisformer.main\n",
    "importlib.reload(Basisformer.main)\n",
    "from Basisformer.main import parse_args, model_setup, log_and_print\n",
    "\n",
    "args = parse_args()\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(f\"cuda:{args.device}\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set up model\n",
    "model = model_setup(args, device)\n",
    "\n",
    "# Log arguments and model\n",
    "log_and_print('Args in experiment:')\n",
    "log_and_print(args)\n",
    "log_and_print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25388088-0a01-4a53-9327-36baeb5ec317",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d34df7-21cb-4303-87f9-d5ec8c03ec8e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33d34df7-21cb-4303-87f9-d5ec8c03ec8e",
    "outputId": "3c5fe222-fb0a-4abc-d84a-489ef41b44b1"
   },
   "outputs": [],
   "source": [
    "import Basisformer.model\n",
    "importlib.reload(Basisformer.model)\n",
    "from Basisformer.model import Basisformer\n",
    "\n",
    "import Basisformer.main\n",
    "importlib.reload(Basisformer.main)\n",
    "from Basisformer.main import train\n",
    "\n",
    "\n",
    "record_dir = os.path.join('records', args.data_path.split('.')[0], 'features_' + args.features,\n",
    "                          'seq_len' + str(args.seq_len) + ',' + 'pred_len' + str(args.pred_len))\n",
    "\n",
    "# Call the train function\n",
    "train(model, train_loader, args, device, record_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886eefd2-4847-4504-a105-5d4fa9396aef",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j59FnuQFNHcV",
   "metadata": {
    "id": "j59FnuQFNHcV"
   },
   "outputs": [],
   "source": [
    "import Basisformer.main\n",
    "importlib.reload(Basisformer.main)\n",
    "from Basisformer.main import test\n",
    "\n",
    "test(model, test_loader, args, device, record_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1803a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "basisformer_train = train\n",
    "basisformer_test = test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ffc309-339b-43e4-ab87-1e1ece074e45",
   "metadata": {},
   "source": [
    "# iTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf6cf6e-e6dc-41fd-a14e-db5a55912360",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 96\n",
    "pred_length = 48\n",
    "label_length = 48\n",
    "curr_model = \"itransformer\"\n",
    "batch_size = 24\n",
    "\n",
    "train_seq_x, train_seq_y = create_sequences(electricity_prices_train_scaled, seq_length, pred_length, label_length, curr_model)\n",
    "train_seq_x_mark, train_seq_y_mark = create_sequences(timestamp_features_train.drop(columns=['Datetime (UTC)']).values, seq_length, pred_length, label_length, curr_model)\n",
    "\n",
    "test_seq_x, test_seq_y = create_sequences(electricity_prices_test_scaled, seq_length, pred_length,label_length, curr_model)\n",
    "test_seq_x_mark, test_seq_y_mark = create_sequences(timestamp_features_test.drop(columns=['Datetime (UTC)']).values, seq_length, pred_length, label_length, curr_model)\n",
    "\n",
    "batch_size = 24\n",
    "\n",
    "# converting sequences to PyTorch DataLoader objects\n",
    "train_loader = create_dataloader(train_seq_x, train_seq_y, train_seq_x_mark, train_seq_y_mark, batch_size, curr_model)\n",
    "test_loader = create_dataloader(test_seq_x, test_seq_y, test_seq_x_mark, test_seq_y_mark, batch_size, curr_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eb6eb6-1970-4b57-8a82-2851c125fa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Sample training sequence x:\", train_seq_x[0])\n",
    "# print(\"Sample training sequence y:\", train_seq_y[0])\n",
    "# print(\"Sample training sequence x mark:\", train_seq_x_mark[0])\n",
    "# print(\"Sample training sequence y mark:\", train_seq_y_mark[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a08286-785d-43a8-960d-bceca7822558",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    print(f\"Batch contains {len(batch)} items:\")\n",
    "    for i, item in enumerate(batch):\n",
    "        print(f\"Item {i}: Shape = {item.shape if torch.is_tensor(item) else 'Not a tensor'}\")\n",
    "    break  # Just print the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dac7e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "# Commented out because model is imported in experiment.py\n",
    "# import iTransformer.Model \n",
    "# importlib.reload(iTransformer.Model)\n",
    "# from iTransformer.Model import Model, Config\n",
    "\n",
    "# Commented because both the imported functions are throwing errors\n",
    "import iTransformer.main\n",
    "importlib.reload(iTransformer.main)\n",
    "from iTransformer.main import parse_args_itrans, long_term_forecast\n",
    "\n",
    "import iTransformer.experiment\n",
    "importlib.reload(iTransformer.experiment)\n",
    "from iTransformer.experiment import Exp_Long_Term_Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0503782-999a-4ec6-a856-82281b291341",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    is_training = 1\n",
    "    model_id = 'iTransformer_train'\n",
    "    model = 'iTransformer'\n",
    "    data = 'all_countries'\n",
    "    features = 'M'\n",
    "    target = 'OT'\n",
    "    freq = 'h'\n",
    "    checkpoints = './checkpoints/'\n",
    "    seq_len = 96\n",
    "    label_len = 48\n",
    "    pred_len = 48\n",
    "    enc_in = 24\n",
    "    dec_in = 24\n",
    "    c_out = 24\n",
    "    d_model = 512\n",
    "    n_heads = 8\n",
    "    e_layers = 2\n",
    "    d_layers = 1\n",
    "    d_ff = 2048\n",
    "    moving_avg = 25\n",
    "    factor = 1\n",
    "    distil = True\n",
    "    dropout = 0.05\n",
    "    embed = 'timeF'\n",
    "    activation = 'gelu'\n",
    "    output_attention = False\n",
    "    do_predict = True\n",
    "    num_workers = 10\n",
    "    itr = 2\n",
    "    train_epochs = 1\n",
    "    batch_size = 32\n",
    "    patience = 3\n",
    "    learning_rate = 0.0001\n",
    "    des = 'test'\n",
    "    loss = 'mse'\n",
    "    lradj = 'type1'\n",
    "    use_amp = False\n",
    "    use_gpu = True if torch.cuda.is_available() else False\n",
    "    gpu = 0\n",
    "    use_multi_gpu = False\n",
    "    devices = '0,1,2,3'\n",
    "    exp_name = 'MTSF'\n",
    "    channel_independence = False\n",
    "    inverse = False\n",
    "    class_strategy = 'projection'\n",
    "    target_root_path = './data'\n",
    "    target_data_path = 'all_countries'\n",
    "    efficient_training = False\n",
    "    use_norm = True\n",
    "    partial_start_index = 0\n",
    "    seed = 2021\n",
    "    p_hidden_dims = [128, 128]\n",
    "    p_hidden_layers = 2\n",
    "\n",
    "args = Args()\n",
    "\n",
    "if args.use_gpu:\n",
    "    if args.use_multi_gpu:\n",
    "        args.devices = args.devices.replace(' ', '')\n",
    "        device_ids = args.devices.split(',')\n",
    "        args.device_ids = [int(id_) for id_ in device_ids]\n",
    "        args.gpu = args.device_ids[0]\n",
    "    else:\n",
    "        torch.cuda.set_device(args.gpu)\n",
    "\n",
    "# Initialize the experiment\n",
    "exp = Exp_Long_Term_Forecast(args) #long_term_forecast(args) #, train_loader, test_loader)\n",
    "\n",
    "# Define the settings\n",
    "setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}'.format(\n",
    "    args.model_id, args.model, args.data, args.features, args.seq_len, args.label_len,\n",
    "    args.pred_len, args.d_model, args.n_heads, args.e_layers, args.d_layers, args.d_ff,\n",
    "    args.factor, args.embed, args.distil, args.des, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca24c83-5c85-41d7-adfb-4b679d9bb9cf",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7177faa3-8546-4f9f-bfdb-5fa3c72fa657",
   "metadata": {},
   "outputs": [],
   "source": [
    "Exp_Long_Term_Forecast.train(self=exp, train_loader=train_loader, setting=setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b770d7dc-c343-4c42-84ee-ab3ab9c3cd1e",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ce0485-c648-48ee-b860-b7bd40e578e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Exp_Long_Term_Forecast.test(self=exp, test_loader=test_loader, setting=setting, test=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1866cae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# itransformer_train = Exp_Long_Term_Forecast.train()\n",
    "# itransformer_test = Exp_Long_Term_Forecast.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa62ae11-381d-4aaf-9e32-ade675824eeb",
   "metadata": {},
   "source": [
    "# Nonstationary Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a8451f-0280-4b1d-83f4-645080a0ebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 96\n",
    "pred_length = 48\n",
    "label_length = 48\n",
    "curr_model = \"ns_autoformer\"\n",
    "\n",
    "train_seq_x, train_seq_y = create_sequences(electricity_prices_train_scaled, seq_length, pred_length, label_length, curr_model)\n",
    "train_seq_x_mark, train_seq_y_mark = create_sequences(timestamp_features_train.drop(columns=['Datetime (UTC)']).values, seq_length, pred_length, label_length, curr_model)\n",
    "\n",
    "\n",
    "test_seq_x, test_seq_y = create_sequences(electricity_prices_test_scaled, seq_length, pred_length,label_length, curr_model)\n",
    "test_seq_x_mark, test_seq_y_mark = create_sequences(timestamp_features_test.drop(columns=['Datetime (UTC)']).values, seq_length, pred_length, label_length, curr_model)\n",
    "\n",
    "batch_size = 24\n",
    "\n",
    "# converting sequences to PyTorch DataLoader objects\n",
    "train_loader = create_dataloader(train_seq_x, train_seq_y, train_seq_x_mark, train_seq_y_mark, batch_size, curr_model)\n",
    "test_loader = create_dataloader(test_seq_x, test_seq_y, test_seq_x_mark, test_seq_y_mark, batch_size, curr_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5613fe15-f1ce-403c-ab95-885a612b9a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    print(f\"Batch contains {len(batch)} items:\")\n",
    "    for i, item in enumerate(batch):\n",
    "        print(f\"Item {i}: Shape = {item.shape if torch.is_tensor(item) else 'Not a tensor'}\")\n",
    "    break  # Just print the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376fd372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import ns_Autoformer.ns_Autoformer\n",
    "importlib.reload(ns_Autoformer.ns_Autoformer)\n",
    "from ns_Autoformer.ns_Autoformer import Model\n",
    "\n",
    "import ns_Autoformer.main\n",
    "importlib.reload(ns_Autoformer.main)\n",
    "from ns_Autoformer.main import parse_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d461a9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ns_Autoformer.main import Exp_Main\n",
    "\n",
    "# setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}'.format(\n",
    "#     args.model_id, args.model, args.features, args.seq_len, args.label_len,\n",
    "#     args.pred_len, args.d_model, args.n_heads, args.e_layers, args.d_layers, args.d_ff,\n",
    "#     args.factor, args.embed, args.distil, args.des, 0)\n",
    "\n",
    "# exp = Exp_Main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eafce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ns_Autoformer.main import Exp_Main\n",
    "\n",
    "class Args:\n",
    "    is_training = 1\n",
    "    model_id = 'ns_autoformer_train'\n",
    "    model = 'ns_Autoformer'\n",
    "    features = 'M'\n",
    "    target = 'OT'\n",
    "    freq = 'h'\n",
    "    checkpoints = './checkpoints/'\n",
    "    seq_len = 96\n",
    "    label_len = 48\n",
    "    pred_len = 48\n",
    "    enc_in = 24\n",
    "    dec_in = 24\n",
    "    c_out = 24\n",
    "    d_model = 512\n",
    "    n_heads = 8\n",
    "    e_layers = 2\n",
    "    d_layers = 1\n",
    "    d_ff = 2048\n",
    "    moving_avg = 25\n",
    "    factor = 1\n",
    "    distil = True\n",
    "    dropout = 0.05\n",
    "    embed = 'timeF'\n",
    "    activation = 'gelu'\n",
    "    output_attention = False\n",
    "    do_predict = True\n",
    "    num_workers = 10\n",
    "    itr = 2\n",
    "    train_epochs = 1\n",
    "    batch_size = 32\n",
    "    patience = 3\n",
    "    learning_rate = 0.0001\n",
    "    des = 'test'\n",
    "    loss = 'mse'\n",
    "    lradj = 'type1'\n",
    "    use_amp = False\n",
    "    use_gpu = True if torch.cuda.is_available() else False\n",
    "    gpu = 0\n",
    "    use_multi_gpu = False\n",
    "    devices = '0,1,2,3'\n",
    "    seed = 2021\n",
    "    p_hidden_dims = [128, 128]\n",
    "    p_hidden_layers = 2\n",
    "\n",
    "args = Args()\n",
    "\n",
    "if args.use_gpu:\n",
    "    if args.use_multi_gpu:\n",
    "        args.devices = args.devices.replace(' ', '')\n",
    "        device_ids = args.devices.split(',')\n",
    "        args.device_ids = [int(id_) for id_ in device_ids]\n",
    "        args.gpu = args.device_ids[0]\n",
    "    else:\n",
    "        torch.cuda.set_device(args.gpu)\n",
    "\n",
    "print('Args in experiment:')\n",
    "print(args)\n",
    "\n",
    "# Initialize the experiment\n",
    "exp = Exp_Main(args)\n",
    "\n",
    "# Define the setting string\n",
    "setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}'.format(\n",
    "    args.model_id, args.model, args.features, args.seq_len, args.label_len,\n",
    "    args.pred_len, args.d_model, args.n_heads, args.e_layers, args.d_layers, args.d_ff,\n",
    "    args.factor, args.embed, args.distil, args.des, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ba12b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ns_Autoformer.main import Exp_Main\n",
    "\n",
    "Exp_Main.train(self=exp, train_loader=train_loader, setting=setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ea5a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "Exp_Main.test(self=exp, test_loader=test_loader, setting=setting, test=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbdc742",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_autoformer_train = Exp_Main.train\n",
    "ns_autoformer_test = Exp_Main.test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c7e547-d9fd-4a30-ae97-27f817377b6e",
   "metadata": {},
   "source": [
    "# Models Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85717e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_models(model_name, train_loader, test_loader,  self, setting, args=None, device=None, record_dir=None):\n",
    "    if model_name == \"basisformer\":\n",
    "        basisformer_train(model_name, train_loader, args, device, record_dir)\n",
    "        results = basisformer_test(model_name, test_loader, args, device, record_dir)\n",
    "    elif model_name == \"itransformer\":\n",
    "        itransformer_train(self, train_loader, setting)\n",
    "        results = itransformer_test(self, test_loader, setting)\n",
    "    elif model_name == \"ns_autoformer\":\n",
    "        ns_autoformer_train(self, train_loader, setting)\n",
    "        results = ns_autoformer_test(self, train_loader, setting)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model name: {model_name}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def compare_models(train_loader, test_loader, args, self, setting, device, record_dir):\n",
    "    models = [\"basisformer\", \"itransformer\", \"ns_autoformer\"]\n",
    "    results = {}\n",
    "    \n",
    "    for model_name in models:\n",
    "        if model_name == \"basisformer\":\n",
    "            results[model_name] = run_models(model_name, train_loader, test_loader, args, self, setting, device, record_dir)\n",
    "        else:\n",
    "            results[model_name] = run_models(model_name, train_loader, test_loader, self, setting)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a93b42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = compare_models(train_loader=train_loader, test_loader=test_loader, args=args, self=exp, setting=setting, device=None, record_dir=None, index=None)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859cd6c1-1569-4378-a015-2e74dcdc430b",
   "metadata": {
    "id": "YCYAEORVlIkJ",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Sythetic Data Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf52e8fc-db3d-4278-9d48-1191b949e600",
   "metadata": {},
   "source": [
    "## Chronos Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab4b902-ca90-486a-9061-e9895762829b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"chronos[training] @ git+https://github.com/amazon-science/chronos-forecasting.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dec950b-f4b5-4876-b8fc-a9814e41dff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python supporting_files_chronos/kernel-synth.py --num-series 500 --max-kernels 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4027a0-2777-4b7d-a2aa-82dd576e5366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.ipc as ipc\n",
    "\n",
    "file_path = 'supporting_files_chronos/kernelsynth-data.arrow'\n",
    "\n",
    "with open(file_path, 'rb') as f:\n",
    "    reader = ipc.RecordBatchFileReader(f)\n",
    "    table = reader.read_all()\n",
    "\n",
    "df_ch = table.to_pandas()\n",
    "\n",
    "print(df_ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f264eb-f9a3-4b7c-a66e-8aeece6d3cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Number of time series\n",
    "num_series = 15\n",
    "# Number of plots per row\n",
    "plots_per_row = 5\n",
    "# Number of rows\n",
    "num_rows = (num_series + plots_per_row - 1) // plots_per_row\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, plots_per_row, figsize=(15, num_rows * 3))\n",
    "\n",
    "for i in range(num_series):\n",
    "    row = i // plots_per_row\n",
    "    col = i % plots_per_row\n",
    "    ax = axes[row, col]\n",
    "    ax.plot(df_ch['target'].iloc[i])\n",
    "    ax.set_title(f'Time Series {i}')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Value')\n",
    "\n",
    "# Remove any empty subplots\n",
    "for j in range(i + 1, num_rows * plots_per_row):\n",
    "    fig.delaxes(axes.flatten()[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9f7069-c8cf-4b8f-95f2-468054c6a2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python supporting_files_chronos/kernel-synth-mult.py --num-series 500 --max-kernels 2 --dimensions 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9f987f-92cc-4e5c-a273-2fb0ecf0c090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.ipc as ipc\n",
    "\n",
    "file_path = 'supporting_files_chronos/kernelsynth-data.arrow'\n",
    "\n",
    "with open(file_path, 'rb') as f:\n",
    "    reader = ipc.RecordBatchFileReader(f)\n",
    "    table = reader.read_all()\n",
    "\n",
    "df_ch_mult = table.to_pandas()\n",
    "\n",
    "print(df_ch_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7970d049-5366-49e7-9748-88810a18167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_ch_mult.head())\n",
    "print(df_ch_mult['target'].head().apply(lambda x: np.array(x).shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3773f5c-f5ad-4c74-8ecf-e232059f8fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot multivariate time series\n",
    "def plot_multivariate_time_series(data, num_rows=3, num_cols=5):\n",
    "    num_series = num_rows * num_cols\n",
    "    time_points = np.arange(len(data[0]) // 3)  # 1024 time points for reshaped data\n",
    "    \n",
    "    fig, axs = plt.subplots(num_rows, num_cols, figsize=(20, num_rows * 4))\n",
    "    axs = axs.flatten()  # Flatten to easily iterate over subplots\n",
    "    \n",
    "    for i in range(num_series):\n",
    "        series = np.array(data[i]).reshape(-1, 3)  # Reshape to [1024, 3]\n",
    "        for j in range(series.shape[1]):\n",
    "            axs[i].plot(time_points, series[:, j], label=f'Dimension {j+1}')\n",
    "        axs[i].set_title(f'Time Series {i+1}')\n",
    "        axs[i].set_xlabel('Time')\n",
    "        axs[i].set_ylabel('Value')\n",
    "        axs[i].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Extract the 'target' column as a list and plot the first 15 multivariate time series\n",
    "plot_multivariate_time_series(df_ch_mult['target'].head(15).tolist(), num_rows=3, num_cols=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a672fb3-0c03-4181-b993-8fa51bf590f8",
   "metadata": {},
   "source": [
    "## DYNOTEARS Causal Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a13bd5-8c01-437f-8347-fe2ac3540708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from causalnex.structure.notears import from_pandas\n",
    "df_str = df.drop(columns=['Datetime (UTC)'])\n",
    "sm = from_pandas(df_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b6112b-9182-4f5f-b5a9-08a1256f7e1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from causalnex.plots import plot_structure, NODE_STYLE, EDGE_STYLE\n",
    "\n",
    "viz = plot_structure(\n",
    "    sm,\n",
    "    all_node_attributes=NODE_STYLE.WEAK,\n",
    "    all_edge_attributes=EDGE_STYLE.WEAK,\n",
    ")\n",
    "\n",
    "viz.toggle_physics(False)\n",
    "viz.show(\"supporting_files_dynotears/01_fully_connected.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b43d61-e4b0-4293-aa32-df6a57a2ae5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.remove_edges_below_threshold(0.8)\n",
    "viz = plot_structure(\n",
    "    sm,\n",
    "    all_node_attributes=NODE_STYLE.WEAK,\n",
    "    all_edge_attributes=EDGE_STYLE.WEAK,\n",
    ")\n",
    "viz.show(\"supporting_files_dynotears/01_thresholded.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9a15d1-6a95-4e85-ab13-31187f8dd0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from causalnex.structure.notears import from_pandas\n",
    "df_str = df.drop(columns=['Datetime (UTC)'])\n",
    "sm = from_pandas(df_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7319feba-69a3-4e82-828e-092c35c80eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.remove_edges_below_threshold(0.8)\n",
    "viz = plot_structure(\n",
    "    sm,\n",
    "    all_node_attributes=NODE_STYLE.WEAK,\n",
    "    all_edge_attributes=EDGE_STYLE.WEAK,\n",
    ")\n",
    "viz.show(\"supporting_files_dynotears/01_thresholded.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c942fe-60ab-413a-b0e4-31ca1187f971",
   "metadata": {},
   "source": [
    "## Granger causality test with nonlinear forecasting methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7ca051-8d45-49ce-941b-a0430f13fc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nonlincausality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f915a324-deac-40c5-bd35-43ca89944098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Feb  7 23:29:32 2022\n",
    "\n",
    "@author: Maciej RosoÅ‚\n",
    "\n",
    "contact: mrosol5@gmail.com, maciej.rosol.dokt@pw.edu.pl\n",
    "\"\"\"\n",
    "#%%\n",
    "import os\n",
    "\n",
    "# os.chdir(os.path.dirname(__file__))\n",
    "import numpy as np\n",
    "##import tensorflow\n",
    "import nonlincausality as nlc\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from nonlincausality.utils import prepare_data_for_prediction, calculate_pred_and_errors\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3187a436-9bba-4d8f-a4ab-d11628aff9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Data generation Y->X\n",
    "np.random.seed(10)\n",
    "y = (\n",
    "    np.cos(np.linspace(0, 20, 10_100))\n",
    "    + np.sin(np.linspace(0, 3, 10_100))\n",
    "    - 0.2 * np.random.random(10_100)\n",
    ")\n",
    "np.random.seed(20)\n",
    "x = 2 * y ** 3 - 5 * y ** 2 + 0.3 * y + 2 - 0.05 * np.random.random(10_100)\n",
    "data = np.vstack([x[:-100], y[100:]]).T\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(data[:, 0], label=\"X\")\n",
    "plt.plot(data[:, 1], label=\"Y\")\n",
    "plt.xlabel(\"Number of sample\")\n",
    "plt.ylabel(\"Signals [AU]\")\n",
    "plt.legend()\n",
    "\n",
    "#%% Test in case of presence of the causality\n",
    "lags = [50, 150]\n",
    "data_train = data[:6000, :]\n",
    "data_val = data[6000:8000, :]\n",
    "data_test = data[8000:, :]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46c29b4-85d6-443c-9b44-90b29d76e404",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = nlc.nonlincausalityNN(\n",
    "    x=data_train,\n",
    "    maxlag=lags,\n",
    "    NN_config=['d','dr','d','dr'],\n",
    "    NN_neurons=[100,0.05,100,0.05],\n",
    "    x_test=data_test,\n",
    "    run=3,\n",
    "    epochs_num=[50, 50],\n",
    "    learning_rate=[0.0001, 0.00001],\n",
    "    batch_size_num=32,\n",
    "    x_val=data_val,\n",
    "    reg_alpha=None,\n",
    "    callbacks=None,\n",
    "    verbose=True,\n",
    "    plot=True,\n",
    ")\n",
    "\n",
    "#%% Example of obtaining the results\n",
    "for lag in lags:\n",
    "    best_model_X = results[lag].best_model_X\n",
    "    best_model_XY = results[lag].best_model_XY\n",
    "\n",
    "    p_value = results[lag].p_value\n",
    "    test_statistic = results[lag]._test_statistic\n",
    "\n",
    "    best_history_X = results[lag].best_history_X\n",
    "    best_history_XY = results[lag].best_history_XY\n",
    "\n",
    "    nlc.plot_history_loss(best_history_X, best_history_XY)\n",
    "    plt.title(\"Lag = %d\" % lag)\n",
    "    best_errors_X = results[lag].best_errors_X\n",
    "    best_errors_XY = results[lag].best_errors_XY\n",
    "\n",
    "    cohens_d = np.abs(\n",
    "        (np.mean(np.abs(best_errors_X)) - np.mean(np.abs(best_errors_XY)))\n",
    "        / np.std([best_errors_X, best_errors_XY])\n",
    "    )\n",
    "    print(\"For lag = %d Cohen's d = %0.3f\" % (lag, cohens_d))\n",
    "    print(f\"Test statistic = {test_statistic} p-value = {p_value}\")\n",
    "\n",
    "    # Using models for prediction\n",
    "    data_X, data_XY = prepare_data_for_prediction(data_test, lag)\n",
    "    X_pred_X = best_model_X.predict(data_X)\n",
    "    X_pred_XY = best_model_XY.predict(data_XY)\n",
    "\n",
    "    # Plot of true X vs X predicted\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    ax[0].plot(data_test[lag:, 0], X_pred_X, \"o\")\n",
    "    ax[0].set_xlabel(\"X test values\")\n",
    "    ax[0].set_ylabel(\"Predicted X values\")\n",
    "    ax[0].set_title(\"Model based on X\")\n",
    "    ax[1].plot(data_test[lag:, 0], X_pred_XY, \"o\")\n",
    "    ax[1].set_xlabel(\"X test values\")\n",
    "    ax[1].set_ylabel(\"Predicted X values\")\n",
    "    ax[1].set_title(\"Model based on X and Y\")\n",
    "    plt.suptitle(\"Lag = %d\" % lag)\n",
    "\n",
    "    # Another way of obtaining predicted values (and errors)\n",
    "    X_pred_X, X_pred_XY, error_X, error_XY = calculate_pred_and_errors(\n",
    "        data_test[lag:, 0], \n",
    "        data_X, \n",
    "        data_XY, \n",
    "        best_model_X, \n",
    "        best_model_XY\n",
    "    )\n",
    "    # Plot of X predicted vs prediction error\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    ax[0].plot(X_pred_X, error_X, \"o\")\n",
    "    ax[0].set_xlabel(\"Predicted X values\")\n",
    "    ax[0].set_ylabel(\"Prediction errors\")\n",
    "    ax[0].set_title(\"Model based on X\")\n",
    "    ax[1].plot(X_pred_XY, error_XY, \"o\")\n",
    "    ax[1].set_xlabel(\"Predicted X values\")\n",
    "    ax[1].set_ylabel(\"Prediction errors\")\n",
    "    ax[1].set_title(\"Model based on X and Y\")\n",
    "    plt.suptitle(\"Lag = %d\" % lag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63afa79c-228d-460e-82f2-6117b2e1924c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Test in case of absence of the causality\n",
    "np.random.seed(30)\n",
    "data_noise = np.vstack([x[:-100], np.random.random(10_000)]).T\n",
    "\n",
    "lags = [50, 150]\n",
    "data_noise_train = data_noise[:6000, :]\n",
    "data_noise_val = data_noise[6000:8000, :]\n",
    "data_noise_test = data_noise[8000:, :]\n",
    "\n",
    "results = nlc.nonlincausalityNN(\n",
    "    x=data_noise_train,\n",
    "    maxlag=lags,\n",
    "    NN_config=['d','dr','d','dr'],\n",
    "    NN_neurons=[100,0.05,100,0.05],\n",
    "    x_test=data_noise_test,\n",
    "    run=3,\n",
    "    epochs_num=[50, 50],\n",
    "    learning_rate=[0.001, 0.0001],\n",
    "    batch_size_num=32,\n",
    "    x_val=data_noise_val,\n",
    "    reg_alpha=None,\n",
    "    callbacks=None,\n",
    "    verbose=True,\n",
    "    plot=True,\n",
    ")\n",
    "\n",
    "#%% Example of obtaining the results\n",
    "for lag in lags:\n",
    "    best_model_X_lag50 = results[lag].best_model_X\n",
    "    best_model_XY_lag50 = results[lag].best_model_XY\n",
    "\n",
    "    p_value = results[lag].p_value\n",
    "    test_statistic = results[lag].test_statistic\n",
    "\n",
    "    best_history_X = results[lag].best_history_X\n",
    "    best_history_XY = results[lag].best_history_XY\n",
    "\n",
    "    nlc.plot_history_loss(best_history_X, best_history_XY)\n",
    "    plt.title(\"Lag = %d\" % lag)\n",
    "\n",
    "    best_errors_X = results[lag].best_errors_X\n",
    "    best_errors_XY = results[lag].best_errors_XY\n",
    "\n",
    "    cohens_d = np.abs(\n",
    "        (np.mean(np.abs(best_errors_X)) - np.mean(np.abs(best_errors_XY)))\n",
    "        / np.std([best_errors_X, best_errors_XY])\n",
    "    )\n",
    "    print(\"For lag = %d Cohen's d = %0.3f\" % (lag, cohens_d))\n",
    "    print(f\"test statistic = {test_statistic} p-value = {p_value}\")\n",
    "#%% Example of the measure of the causality change over time\n",
    "\n",
    "data_test_measure = copy.copy(data_test)\n",
    "np.random.seed(30)\n",
    "data_test_measure[:1000, 1] = np.random.random(1000)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(data_test_measure[:, 0], label=\"X\")\n",
    "plt.plot(data_test_measure[:, 1], label=\"Y\")\n",
    "plt.xlabel(\"Number of sample\")\n",
    "plt.ylabel(\"Signals [AU]\")\n",
    "plt.legend()\n",
    "\n",
    "results = nlc.nonlincausalitymeasureNN(\n",
    "    x=data_train,\n",
    "    maxlag=lags,\n",
    "    window=100,\n",
    "    step=1,\n",
    "    NN_config=['d','dr','d','dr'],\n",
    "    NN_neurons=[100,0.05,100,0.05],\n",
    "    x_test=data_test_measure,\n",
    "    run=3,\n",
    "    epochs_num=[50,50],\n",
    "    learning_rate=[0.0001, 0.00001],\n",
    "    batch_size_num=32,\n",
    "    x_val=data_val,\n",
    "    verbose=True,\n",
    "    plot=True,\n",
    ")\n",
    "\n",
    "\n",
    "#%% Example of usage for conditional analysis\n",
    "np.random.seed(30)\n",
    "z = np.random.random([10_000, 2])\n",
    "\n",
    "z_train = z[:6000, :]\n",
    "z_val = z[6000:8000, :]\n",
    "z_test = z[8000:, :]\n",
    "\n",
    "results_conditional = nlc.nonlincausalityNN(\n",
    "    x=data_train,\n",
    "    maxlag=lags,\n",
    "    NN_config=['d','dr','d','dr'],\n",
    "    NN_neurons=[100,0.05,100,0.05],\n",
    "    x_test=data_test,\n",
    "    run=1,\n",
    "    z=z_train,\n",
    "    z_test=z_test,\n",
    "    epochs_num=[50, 50],\n",
    "    learning_rate=[0.0001, 0.00001],\n",
    "    batch_size_num=32,\n",
    "    x_val=data_val,\n",
    "    z_val=z_val,\n",
    "    reg_alpha=None,\n",
    "    callbacks=None,\n",
    "    verbose=True,\n",
    "    plot=True,\n",
    ")\n",
    "# %% Exaple of the usage the package with Scikit-learn model\n",
    "\n",
    "parametres = {\n",
    "    'kernel':['poly', 'rbf'],\n",
    "    'C':[0.01,0.1,1], \n",
    "    'epsilon':[0.01,0.1,1.]\n",
    "}\n",
    "results_skl = nlc.nonlincausality_sklearn(    \n",
    "    x=data_train,\n",
    "    sklearn_model=SVR,\n",
    "    maxlag=lags,\n",
    "    params=parametres,\n",
    "    x_test=data_test,\n",
    "    x_val=data_val,\n",
    "    plot=True)\n",
    "\n",
    "#%% Example of usage other functions for causality analysis\n",
    "\n",
    "# ARIMA/ARIMAX models\n",
    "results_ARIMA = nlc.nonlincausalityARIMA(x=data_train[::10], maxlag=[5,15], x_test=data_test[::10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c49845",
   "metadata": {},
   "source": [
    "# Device Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2fa8cd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(f\"cuda:{args.device}\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8548e1f7",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f6a0895",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 96\n",
    "pred_length = 48\n",
    "label_length = 48\n",
    "curr_model = \"itransformer\"\n",
    "\n",
    "train_seq_x, train_seq_y = create_sequences(electricity_prices_train_scaled, seq_length, pred_length, label_length, curr_model)\n",
    "train_seq_x_mark, train_seq_y_mark = create_sequences(timestamp_features_train.drop(columns=['Datetime (UTC)']).values, seq_length, pred_length, label_length, curr_model)\n",
    "\n",
    "\n",
    "test_seq_x, test_seq_y = create_sequences(electricity_prices_test_scaled, seq_length, pred_length,label_length, curr_model)\n",
    "test_seq_x_mark, test_seq_y_mark = create_sequences(timestamp_features_test.drop(columns=['Datetime (UTC)']).values, seq_length, pred_length, label_length, curr_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ce08587",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 24\n",
    "\n",
    "# converting sequences to PyTorch DataLoader objects\n",
    "train_loader = create_dataloader(train_seq_x, train_seq_y, train_seq_x_mark, train_seq_y_mark, batch_size, curr_model)\n",
    "test_loader = create_dataloader(test_seq_x, test_seq_y, test_seq_x_mark, test_seq_y_mark, batch_size, curr_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652b7a2b",
   "metadata": {},
   "source": [
    "# Unification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10670e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit (model, train_flag, test_flag, train_loader=None, test_loader=None, pretrained_model=None):\n",
    "    '''Fits a transformer model to the train and/or test loaders\n",
    "    \n",
    "    model - \"basis_former\", \"itransformer\", \"ns_autoformer\"\n",
    "    \n",
    "    train_flag: typ(bool) - True: to train the model on train_loader, False: if pretrained_model is passed\n",
    "    \n",
    "    test_flag: typ(bool) - True: to test on test_loader, False: if only training\n",
    "    \n",
    "    pretrained_model - pass a pretrained model if available to be fitted on a test_loader. \n",
    "    eg. fit(basis_former, train_flag=False, test_flag=True, test_loader=test_loader, pretrained_model=model)\n",
    "    '''\n",
    "    \n",
    "    if curr_model == 'basis_former':\n",
    "        # Code for Basisforme\n",
    "\n",
    "        import Basisformer.model\n",
    "        importlib.reload(Basisformer.model)\n",
    "        from Basisformer.model import Basisformer\n",
    "\n",
    "        import Basisformer.main\n",
    "        importlib.reload(Basisformer.main)\n",
    "        from Basisformer.main import parse_args, model_setup, log_and_print\n",
    "\n",
    "        args = parse_args()\n",
    "        \n",
    "        if pretrained_model == None:\n",
    "            # Set up model\n",
    "            model = model_setup(args, device)\n",
    "\n",
    "        else:\n",
    "            model = pretrained_model\n",
    "\n",
    "        # Log arguments and model\n",
    "        log_and_print('Args in experiment:')\n",
    "        log_and_print(args)\n",
    "        log_and_print(model)\n",
    "        \n",
    "        if train_flag:\n",
    "            import Basisformer.model\n",
    "            importlib.reload(Basisformer.model)\n",
    "            from Basisformer.model import Basisformer\n",
    "\n",
    "            import Basisformer.main\n",
    "            importlib.reload(Basisformer.main)\n",
    "            from Basisformer.main import train\n",
    "\n",
    "\n",
    "            record_dir = os.path.join('records', args.data_path.split('.')[0], 'features_' + args.features,\n",
    "                                    'seq_len' + str(args.seq_len) + ',' + 'pred_len' + str(args.pred_len))\n",
    "            \n",
    "            if train_loader == None:\n",
    "                return 'train_loader not found'\n",
    "\n",
    "            # Call the train function\n",
    "            train(model, train_loader, args, device, record_dir)\n",
    "            \n",
    "        else:\n",
    "            if pretrained_model == None:\n",
    "                return 'model not found which is required for testing'\n",
    "            \n",
    "        if test_flag :\n",
    "            import Basisformer.main\n",
    "            importlib.reload(Basisformer.main)\n",
    "            from Basisformer.main import test\n",
    "            \n",
    "            if test_loader == None:\n",
    "                return 'test_loader not found'\n",
    "\n",
    "            test(model, test_loader, args, device, record_dir)\n",
    "        return model\n",
    "            \n",
    "    \n",
    "    elif curr_model == 'itransformer':\n",
    "        # code for itransformer\n",
    "        \n",
    "        import iTransformer.experiment\n",
    "        importlib.reload(iTransformer.experiment)\n",
    "        from iTransformer.experiment import Exp_Long_Term_Forecast\n",
    "        \n",
    "        from iTransformer import Model\n",
    "        \n",
    "        class Args:\n",
    "            is_training = 1\n",
    "            model_id = 'iTransformer_train'\n",
    "            model = 'iTransformer'\n",
    "            data = 'all_countries'\n",
    "            features = 'M'\n",
    "            target = 'OT'\n",
    "            freq = 'h'\n",
    "            checkpoints = './checkpoints/'\n",
    "            seq_len = 96\n",
    "            label_len = 48\n",
    "            pred_len = 48\n",
    "            enc_in = 24\n",
    "            dec_in = 24\n",
    "            c_out = 24\n",
    "            d_model = 512\n",
    "            n_heads = 8\n",
    "            e_layers = 2\n",
    "            d_layers = 1\n",
    "            d_ff = 2048\n",
    "            moving_avg = 25\n",
    "            factor = 1\n",
    "            distil = True\n",
    "            dropout = 0.05\n",
    "            embed = 'timeF'\n",
    "            activation = 'gelu'\n",
    "            output_attention = False\n",
    "            do_predict = True\n",
    "            num_workers = 10\n",
    "            itr = 2\n",
    "            train_epochs = 1\n",
    "            batch_size = 32\n",
    "            patience = 3\n",
    "            learning_rate = 0.0001\n",
    "            des = 'test'\n",
    "            loss = 'mse'\n",
    "            lradj = 'type1'\n",
    "            use_amp = False\n",
    "            use_gpu = True if torch.cuda.is_available() else False\n",
    "            gpu = 0\n",
    "            use_multi_gpu = False\n",
    "            devices = '0,1,2,3'\n",
    "            exp_name = 'MTSF'\n",
    "            channel_independence = False\n",
    "            inverse = False\n",
    "            class_strategy = 'projection'\n",
    "            target_root_path = './data'\n",
    "            target_data_path = 'all_countries'\n",
    "            efficient_training = False\n",
    "            use_norm = True\n",
    "            partial_start_index = 0\n",
    "            seed = 2021\n",
    "            p_hidden_dims = [128, 128]\n",
    "            p_hidden_layers = 2\n",
    "\n",
    "        args = Args()\n",
    "        \n",
    "        if pretrained_model == None:\n",
    "            # Initialize the experiment\n",
    "            exp = Exp_Long_Term_Forecast(args)\n",
    "\n",
    "        else:\n",
    "            return 'pretrained not valid for iTransformer and ns_autoformer'\n",
    "\n",
    "        # Define the settings\n",
    "        setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}'.format(\n",
    "            args.model_id, args.model, args.data, args.features, args.seq_len, args.label_len,\n",
    "            args.pred_len, args.d_model, args.n_heads, args.e_layers, args.d_layers, args.d_ff,\n",
    "            args.factor, args.embed, args.distil, args.des, 0)\n",
    "        \n",
    "        if train_flag:\n",
    "            Exp_Long_Term_Forecast.train(self=exp, train_loader=train_loader, setting=setting)\n",
    "        \n",
    "        if test_flag:\n",
    "            Exp_Long_Term_Forecast.test(self=exp, test_loader=test_loader, setting=setting, test=0)\n",
    "        return exp.model\n",
    "    \n",
    "    elif curr_model == 'ns_autoformer':\n",
    "        # code for itransformer\n",
    "        import ns_Autoformer.ns_Autoformer\n",
    "        importlib.reload(ns_Autoformer.ns_Autoformer)\n",
    "        from ns_Autoformer.ns_Autoformer import Model\n",
    "\n",
    "        # import ns_Autoformer.main\n",
    "        # importlib.reload(ns_Autoformer.main)\n",
    "        # from ns_Autoformer.main import parse_args\n",
    "        \n",
    "        from ns_Autoformer.main import Exp_Main\n",
    "\n",
    "        class Args:\n",
    "            is_training = 1\n",
    "            model_id = 'ns_autoformer_train'\n",
    "            model = 'ns_Autoformer'\n",
    "            features = 'M'\n",
    "            target = 'OT'\n",
    "            freq = 'h'\n",
    "            checkpoints = './checkpoints/'\n",
    "            seq_len = 96\n",
    "            label_len = 48\n",
    "            pred_len = 48\n",
    "            enc_in = 24\n",
    "            dec_in = 24\n",
    "            c_out = 24\n",
    "            d_model = 512\n",
    "            n_heads = 8\n",
    "            e_layers = 2\n",
    "            d_layers = 1\n",
    "            d_ff = 2048\n",
    "            moving_avg = 25\n",
    "            factor = 1\n",
    "            distil = True\n",
    "            dropout = 0.05\n",
    "            embed = 'timeF'\n",
    "            activation = 'gelu'\n",
    "            output_attention = False\n",
    "            do_predict = True\n",
    "            num_workers = 10\n",
    "            itr = 2\n",
    "            train_epochs = 1\n",
    "            batch_size = 32\n",
    "            patience = 3\n",
    "            learning_rate = 0.0001\n",
    "            des = 'test'\n",
    "            loss = 'mse'\n",
    "            lradj = 'type1'\n",
    "            use_amp = False\n",
    "            use_gpu = True if torch.cuda.is_available() else False\n",
    "            gpu = 0\n",
    "            use_multi_gpu = False\n",
    "            devices = '0,1,2,3'\n",
    "            seed = 2021\n",
    "            p_hidden_dims = [128, 128]\n",
    "            p_hidden_layers = 2\n",
    "\n",
    "        args = Args()\n",
    "\n",
    "        # if args.use_gpu:\n",
    "        #     if args.use_multi_gpu:\n",
    "        #         args.devices = args.devices.replace(' ', '')\n",
    "        #         device_ids = args.devices.split(',')\n",
    "        #         args.device_ids = [int(id_) for id_ in device_ids]\n",
    "        #         args.gpu = args.device_ids[0]\n",
    "        #     else:\n",
    "        #         torch.cuda.set_device(args.gpu)\n",
    "\n",
    "        # print('Args in experiment:')\n",
    "        # print(args)\n",
    "\n",
    "        # Initialize the experiment\n",
    "        exp = Exp_Main(args)\n",
    "\n",
    "        # Define the setting string\n",
    "        setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}'.format(\n",
    "            args.model_id, args.model, args.features, args.seq_len, args.label_len,\n",
    "            args.pred_len, args.d_model, args.n_heads, args.e_layers, args.d_layers, args.d_ff,\n",
    "            args.factor, args.embed, args.distil, args.des, 0)\n",
    "        \n",
    "        if train_flag:\n",
    "            Exp_Main.train(self=exp, train_loader=train_loader, setting=setting)\n",
    "        \n",
    "        if test_flag:\n",
    "            Exp_Main.test(self=exp, test_loader=test_loader, setting=setting, test=0)\n",
    "        return exp.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55134ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use CPU\n",
      "Epoch: 1 cost time: 48.523653984069824\n",
      "Epoch: 1, Steps: 65 | Train Loss: 0.7034816\n",
      "Validation loss decreased (inf --> 0.703482).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "test shape: (12, 24, 48, 24) (12, 24, 48, 24)\n",
      "test shape: (288, 48, 24) (288, 48, 24)\n",
      "mse:1.3615845441818237, mae:0.847229540348053\n"
     ]
    }
   ],
   "source": [
    "iTransformer_train_test = fit(model=curr_model, train_flag=True, test_flag=True, train_loader=train_loader, test_loader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c7e457",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfcdf33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
