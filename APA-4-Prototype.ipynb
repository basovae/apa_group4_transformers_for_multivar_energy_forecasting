{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecd9ee4f-6424-4b21-af1a-e78b6d8a1d3a",
   "metadata": {},
   "source": [
    "Paper link: https://openreview.net/forum?id=xx3qRKvG0T\n",
    "\n",
    "\n",
    "Repo link: https://github.com/nzl5116190/Basisformer?tab=readme-ov-file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfa9275-ad18-4fbc-9070-d289d4ff159c",
   "metadata": {},
   "source": [
    "Summary of the Contents\n",
    "main.py\n",
    "The main.py file includes the main execution logic for training and validating the model. Here's a brief overview of its structure:\n",
    "\n",
    "Imports: Various libraries including PyTorch, NumPy, and custom modules like Basisformer from model, metric from evaluate_tool, and plot_seq_feature from pyplot.\n",
    "Validation Function (vali): Evaluates the model on validation data and logs the results.\n",
    "Training Function (train): Handles the model training loop, including data loading, optimizer setup, and training steps.\n",
    "Main Execution: Parses command-line arguments, sets random seeds for reproducibility, initializes the model and other components, and starts the training and validation process.\n",
    "model.py\n",
    "The model.py file contains the definition of the Basisformer model, including its forward pass logic. Here's a brief overview:\n",
    "\n",
    "Imports: PyTorch and other necessary libraries.\n",
    "Model Definition (Basisformer): Defines the architecture of the model, including its various layers and the forward pass logic.\n",
    "Forward Pass: The forward method includes detailed steps for processing the input through the model, including optional training-specific operations like computing loss components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "47513faa-31fa-4d81-974a-0e9aeb726830",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1: Setup\n",
    "import argparse\n",
    "import torch\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "import random\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from adabelief_pytorch import AdaBelief\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# Add the parent directory to the path if needed\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Adjust the import statements to match your project structure\n",
    "from data_provider.data_factory import data_provider\n",
    "from model import Basisformer\n",
    "from evaluate_tool import metric\n",
    "from pyplot import plot_seq_feature\n",
    "\n",
    "# Function to set seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)  # Set seed for reproducibility\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define arguments manually\n",
    "class Args:\n",
    "    features = 'S'  # Set to your required value\n",
    "    seq_len = 96  # Set to your required value\n",
    "    pred_len = 24  # Set to your required value\n",
    "    train_epochs = 10  # Set to your required value\n",
    "    learning_rate = 0.001  # Set to your required value\n",
    "    log_dir = './logs'  # Set to your required value\n",
    "\n",
    "args = Args()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "be200143-3d4a-4b52-98c2-37d8e7dc3d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Country ISO3 Code       Datetime (UTC)     Datetime (Local)  \\\n",
      "0  Austria       AUT  2015-01-01 00:00:00  2015-01-01 01:00:00   \n",
      "1  Austria       AUT  2015-01-01 01:00:00  2015-01-01 02:00:00   \n",
      "2  Austria       AUT  2015-01-01 02:00:00  2015-01-01 03:00:00   \n",
      "3  Austria       AUT  2015-01-01 03:00:00  2015-01-01 04:00:00   \n",
      "4  Austria       AUT  2015-01-01 04:00:00  2015-01-01 05:00:00   \n",
      "\n",
      "   Price (EUR/MWhe)  \n",
      "0             17.93  \n",
      "1             15.17  \n",
      "2             16.38  \n",
      "3             17.38  \n",
      "4             16.38  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the dataset\n",
    "file_path = '/Users/ekaterinabasova/Desktop/HU/Projects/APA_Transformers/apa_group4_transformers_for_multivar_energy_forecasting/european_wholesale_electricity_price_data_hourly/all_countries.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows to understand its structure\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "66d70339-a334-40aa-b41f-fdbe18d29942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Country  Price (EUR/MWhe)  year  month  day      hour\n",
      "0      0.0          0.115096   0.0    0.0  0.0  0.000000\n",
      "1      0.0          0.114482   0.0    0.0  0.0  0.043478\n",
      "2      0.0          0.114751   0.0    0.0  0.0  0.086957\n",
      "3      0.0          0.114973   0.0    0.0  0.0  0.130435\n",
      "4      0.0          0.114751   0.0    0.0  0.0  0.173913\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(data):\n",
    "    # Handle missing values\n",
    "    data = data.fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    # Encode categorical data for Country\n",
    "    data['Country'] = data['Country'].astype('category').cat.codes\n",
    "\n",
    "    # Handle datetime\n",
    "    data['Datetime (UTC)'] = pd.to_datetime(data['Datetime (UTC)'])\n",
    "    data['year'] = data['Datetime (UTC)'].dt.year\n",
    "    data['month'] = data['Datetime (UTC)'].dt.month\n",
    "    data['day'] = data['Datetime (UTC)'].dt.day\n",
    "    data['hour'] = data['Datetime (UTC)'].dt.hour\n",
    "\n",
    "    # Drop original datetime columns and ISO3 Code\n",
    "    data = data.drop(['Datetime (UTC)', 'Datetime (Local)', 'ISO3 Code'], axis=1)\n",
    "\n",
    "    # Normalize data (example: min-max scaling)\n",
    "    scaler = MinMaxScaler()\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    data_scaled = pd.DataFrame(data_scaled, columns=data.columns)\n",
    "    \n",
    "    return data_scaled\n",
    "\n",
    "\n",
    "# Preprocess the data\n",
    "data_preprocessed = preprocess_data(data)\n",
    "\n",
    "# Display the first few rows of the preprocessed data\n",
    "print(data_preprocessed.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be08220-e730-4ec6-80c7-6cf8b0cd6818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data provider function\n",
    "def data_provider(args, flag='train'):\n",
    "    file_path = '/Users/ekaterinabasova/Desktop/HU/Projects/APA_Transformers/apa_group4_transformers_for_multivar_energy_forecasting/european_wholesale_electricity_price_data_hourly/all_countries.csv'\n",
    "    if flag == 'train':\n",
    "        data = pd.read_csv(file_path)\n",
    "        data = preprocess_data(data)\n",
    "        \n",
    "        # Assuming the data is now a NumPy array after preprocessing\n",
    "        data = data.values\n",
    "        \n",
    "        # Split data into features and labels\n",
    "        X = data[:, :-1]  # All columns except the last one as features\n",
    "        y = data[:, -1]   # Last column as label\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "        # Create DataLoader\n",
    "        dataset = torch.utils.data.TensorDataset(X, y)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        return dataset, dataloader\n",
    "\n",
    "\n",
    "    # Implement validation and test loading similarly\n",
    "    # ...\n",
    "\n",
    "# Load training data\n",
    "train_set, train_loader = data_provider(args, \"train\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7645fd6a-8106-4a4b-b1f0-ff41f4233c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 2: Data Preparation\n",
    "# Assuming data_provider and related data functions are defined in data_provider.py\n",
    "\n",
    "def load_data(args, flag='train'):\n",
    "    return data_provider(args, flag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2c9687c-9e55-45ec-9270-4b16829bbed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3: Model Definition\n",
    "# Load the model definition from model.py\n",
    "class Basisformer(nn.Module):\n",
    "    def __init__(self,seq_len,pred_len,d_model,heads,basis_nums,block_nums,bottle,map_bottleneck,device,tau):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.k = heads\n",
    "        self.N = basis_nums\n",
    "        self.coefnet = Coefnet(blocks=block_nums,d_model=d_model,heads=heads)\n",
    "            \n",
    "        self.pred_len = pred_len\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.MLP_x = MLP_bottle(seq_len,heads * int(seq_len/heads),int(seq_len/bottle))\n",
    "        self.MLP_y = MLP_bottle(pred_len,heads * int(pred_len/heads),int(pred_len/bottle))\n",
    "        self.MLP_sx = MLP_bottle(heads * int(seq_len/heads),seq_len,int(seq_len/bottle))\n",
    "        self.MLP_sy = MLP_bottle(heads * int(pred_len/heads),pred_len,int(pred_len/bottle))\n",
    "        \n",
    "        self.project1 = wn(nn.Linear(seq_len,d_model))\n",
    "        self.project2 = wn(nn.Linear(seq_len,d_model))\n",
    "        self.project3 = wn(nn.Linear(pred_len,d_model))\n",
    "        self.project4 = wn(nn.Linear(pred_len,d_model))\n",
    "        self.criterion1 = nn.MSELoss()\n",
    "        self.criterion2 = nn.L1Loss(reduction='none')\n",
    "        \n",
    "        self.device = device\n",
    "                        \n",
    "        # smooth array\n",
    "        arr = torch.zeros((seq_len+pred_len-2,seq_len+pred_len))\n",
    "        for i in range(seq_len+pred_len-2):\n",
    "            arr[i,i]=-1\n",
    "            arr[i,i+1] = 2\n",
    "            arr[i,i+2] = -1\n",
    "        self.smooth_arr = arr.to(device)\n",
    "        self.map_MLP = MLP_bottle(1,self.N*(self.seq_len+self.pred_len),map_bottleneck,bias=True)\n",
    "        self.tau = tau\n",
    "        self.epsilon = 1E-5\n",
    "        \n",
    "    def forward(self,x,mark,y=None,train=True,y_mark=None):\n",
    "        mean_x = x.mean(dim=1,keepdim=True)\n",
    "        std_x = x.std(dim=1,keepdim=True)\n",
    "        feature = (x - mean_x) / (std_x + self.epsilon)\n",
    "        B,L,C = feature.shape\n",
    "        feature = feature.permute(0,2,1)\n",
    "        feature = self.project1(feature)   #(B,C,d)\n",
    "        \n",
    "        m = self.map_MLP(mark[:,0].unsqueeze(1)).reshape(B,self.seq_len + self.pred_len,self.N)\n",
    "        m = m / torch.sqrt(torch.sum(m**2,dim=1,keepdim=True)+self.epsilon)\n",
    "        \n",
    "        raw_m1 = m[:,:self.seq_len].permute(0,2,1)  #(B,L,N)\n",
    "        raw_m2 = m[:,self.seq_len:].permute(0,2,1)   #(B,L',N)\n",
    "        m1 = self.project2(raw_m1)    #(B,N,d)\n",
    "        \n",
    "        score,attn_x1,attn_x2 = self.coefnet(m1,feature)    #(B,k,C,N)\n",
    "\n",
    "        base = self.MLP_y(raw_m2).reshape(B,self.N,self.k,-1).permute(0,2,1,3)   #(B,k,N,L/k)\n",
    "        out = torch.matmul(score,base).permute(0,2,1,3).reshape(B,C,-1)  #(B,C,k * (L/k))\n",
    "        out = self.MLP_sy(out).reshape(B,C,-1).permute(0,2,1)   #（BC,L）\n",
    "        \n",
    "        output = out * (std_x + self.epsilon) + mean_x\n",
    "\n",
    "        #loss\n",
    "        if train:\n",
    "            l_smooth = torch.einsum('xl,bln->xbn',self.smooth_arr,m)\n",
    "            l_smooth = abs(l_smooth).mean()\n",
    "            # l_smooth = self.criterion1(l_smooth,torch.zeros_like(l_smooth))\n",
    "            \n",
    "            # #back\n",
    "            mean_y = y.mean(dim=1,keepdim=True)\n",
    "            std_y = y.std(dim=1,keepdim=True)\n",
    "            feature_y_raw = (y - mean_y) / (std_y + self.epsilon)\n",
    "            \n",
    "            feature_y = feature_y_raw.permute(0,2,1)\n",
    "            feature_y = self.project3(feature_y)   #(BC,d)\n",
    "            m2 = self.project4(raw_m2)    #(N,d)\n",
    "            \n",
    "            score_y,attn_y1,attn_y2 = self.coefnet(m2,feature_y)    #(B,k,C,N)\n",
    "            logit_q = score.permute(0,2,3,1) #(B,C,N,k)\n",
    "            logit_k = score_y.permute(0,2,3,1) #(B,C,N,k)\n",
    "\n",
    "            # l_pos = torch.bmm(logit_q.view(-1,1,self.k), logit_k.view(-1,self.k,1)).reshape(-1,1)  #(B*C*N,1,1)\n",
    "            l_neg = torch.bmm(logit_q.reshape(-1,self.N,self.k), logit_k.reshape(-1,self.N,self.k).permute(0,2,1)).reshape(-1,self.N) # (B,C*N,N)\n",
    "\n",
    "            labels = torch.arange(0,self.N,1,dtype=torch.long).unsqueeze(0).repeat(B*C,1).reshape(-1)\n",
    "\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "            cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "            l_entropy = cross_entropy_loss(l_neg/self.tau, labels)           \n",
    "            \n",
    "            return output,l_entropy,l_smooth,attn_x1,attn_x2,attn_y1,attn_y2\n",
    "        else:\n",
    "            # #back\n",
    "            mean_y = y.mean(dim=1,keepdim=True)\n",
    "            std_y = y.std(dim=1,keepdim=True)\n",
    "            feature_y_raw = (y - mean_y) / (std_y + self.epsilon)\n",
    "            \n",
    "            feature_y = feature_y_raw.permute(0,2,1)\n",
    "            feature_y = self.project3(feature_y)   #(BC,d)\n",
    "            m2 = self.project4(raw_m2)    #(N,d)\n",
    "            \n",
    "            score_y,attn_y1,attn_y2 = self.coefnet(m2,feature_y)    #(B,k,C,N)\n",
    "            return output,m,attn_x1,attn_x2,attn_y1,attn_y2      \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bde82839-4052-470c-8aec-c4a42f40abf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 4: Training and Validation\n",
    "\n",
    "def vali(vali_data, vali_loader, criterion, epoch, writer, flag='vali'):\n",
    "    total_loss = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark, index) in enumerate(vali_loader):\n",
    "            batch_x = batch_x.float().to(device)\n",
    "            batch_y = batch_y.float()\n",
    "\n",
    "            f_dim = -1 if args.features == 'MS' else 0\n",
    "            batch_y = batch_y[:, -args.pred_len:, f_dim:].to(device)\n",
    "            batch_y_mark = batch_y_mark.float().to(device)\n",
    "\n",
    "            real_batch_x = batch_x\n",
    "\n",
    "            outputs, m, attn_x1, attn_x2, attn_y1, attn_y2 = model(batch_x, index.float().to(device), batch_y, train=False, y_mark=batch_y_mark)\n",
    "\n",
    "            pred = outputs.detach().cpu()\n",
    "            true = batch_y.detach().cpu()\n",
    "\n",
    "            loss_raw = criterion(pred, true)\n",
    "            loss = loss_raw.mean()\n",
    "\n",
    "            total_loss.append(loss)\n",
    "\n",
    "            if i == 0:\n",
    "                fig = plot_seq_feature(outputs, batch_y, real_batch_x, flag)\n",
    "                writer.add_figure(\"figure_{}\".format(flag), fig, global_step=epoch)\n",
    "\n",
    "    total_loss = np.average(total_loss)\n",
    "    model.train()\n",
    "    return total_loss\n",
    "\n",
    "def train():\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print('[Info] Number of parameters: {}'.format(num_params))\n",
    "    train_set, train_loader = load_data(args, \"train\")\n",
    "    vali_data, vali_loader = load_data(args, flag='val')\n",
    "    test_data, test_loader = load_data(args, flag='test')\n",
    "\n",
    "    para1 = [param for name, param in model.named_parameters() if 'smooth_arr' not in name]\n",
    "    para2 = [param for name, param in model.named_parameters() if 'smooth_arr' in name]\n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {'params': para1},\n",
    "        {'params': para2, 'lr': args.learning_rate * 0.1}\n",
    "    ], lr=args.learning_rate)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    writer = SummaryWriter(log_dir=args.log_dir)\n",
    "\n",
    "    for epoch in range(args.train_epochs):\n",
    "        model.train()\n",
    "        total_loss = []\n",
    "\n",
    "        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark, index) in enumerate(train_loader):\n",
    "            batch_x = batch_x.float().to(device)\n",
    "            batch_y = batch_y.float()\n",
    "\n",
    "            f_dim = -1 if args.features == 'MS' else 0\n",
    "            batch_y = batch_y[:, -args.pred_len:, f_dim:].to(device)\n",
    "            batch_y_mark = batch_y_mark.float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs, l_entropy, l_smooth, attn_x1, attn_x2, attn_y1, attn_y2 = model(batch_x, index.float().to(device), batch_y, y_mark=batch_y_mark)\n",
    "\n",
    "            pred = outputs.detach().cpu()\n",
    "            true = batch_y.detach().cpu()\n",
    "\n",
    "            loss_raw = criterion(pred, true)\n",
    "            loss = loss_raw.mean() + l_entropy + l_smooth\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss.append(loss.item())\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(\"Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}\".format(epoch + 1, args.train_epochs, i + 1, len(train_loader), loss.item()))\n",
    "\n",
    "        train_loss = np.average(total_loss)\n",
    "        vali_loss = vali(vali_data, vali_loader, criterion, epoch, writer, flag='vali')\n",
    "        print(\"Epoch: {} Train Loss: {} Validation Loss: {}\".format(epoch + 1, train_loss, vali_loss))\n",
    "        writer.add_scalar('train_loss', train_loss, global_step=epoch)\n",
    "        writer.add_scalar('vali_loss', vali_loss, global_step=epoch)\n",
    "\n",
    "    writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e26feebc-35c9-48b4-9a7e-dfe0d5b4bb68",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (4293515873.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[25], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    train()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "## Step 5: Execution\n",
    "\n",
    "    train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (basisformer_env)",
   "language": "python",
   "name": "basisformer_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
