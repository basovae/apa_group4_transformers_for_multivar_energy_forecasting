{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27318704-5eff-4e94-95e4-4b5cb8f4860e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.tseries import offsets\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "\n",
    "\n",
    "class TimeFeature:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \"()\"\n",
    "\n",
    "\n",
    "class SecondOfMinute(TimeFeature):\n",
    "    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.second / 59.0 - 0.5\n",
    "\n",
    "\n",
    "class MinuteOfHour(TimeFeature):\n",
    "    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.minute / 59.0 - 0.5\n",
    "\n",
    "\n",
    "class HourOfDay(TimeFeature):\n",
    "    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.hour / 23.0 - 0.5\n",
    "\n",
    "\n",
    "class DayOfWeek(TimeFeature):\n",
    "    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.dayofweek / 6.0 - 0.5\n",
    "\n",
    "\n",
    "class DayOfMonth(TimeFeature):\n",
    "    \"\"\"Day of month encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.day - 1) / 30.0 - 0.5\n",
    "\n",
    "\n",
    "class DayOfYear(TimeFeature):\n",
    "    \"\"\"Day of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.dayofyear - 1) / 365.0 - 0.5\n",
    "\n",
    "\n",
    "class MonthOfYear(TimeFeature):\n",
    "    \"\"\"Month of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.month - 1) / 11.0 - 0.5\n",
    "\n",
    "\n",
    "class WeekOfYear(TimeFeature):\n",
    "    \"\"\"Week of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.isocalendar().week - 1) / 52.0 - 0.5\n",
    "\n",
    "\n",
    "def time_features_from_frequency_str(freq_str: str) -> List[TimeFeature]:\n",
    "    \"\"\"\n",
    "    Returns a list of time features that will be appropriate for the given frequency string.\n",
    "    Parameters\n",
    "    ----------\n",
    "    freq_str\n",
    "        Frequency string of the form [multiple][granularity] such as \"12H\", \"5min\", \"1D\" etc.\n",
    "    \"\"\"\n",
    "\n",
    "    features_by_offsets = {\n",
    "        offsets.YearEnd: [],\n",
    "        offsets.QuarterEnd: [MonthOfYear],\n",
    "        offsets.MonthEnd: [MonthOfYear],\n",
    "        offsets.Week: [DayOfMonth, WeekOfYear],\n",
    "        offsets.Day: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.BusinessDay: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Hour: [HourOfDay, DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Minute: [\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "        offsets.Second: [\n",
    "            SecondOfMinute,\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    offset = to_offset(freq_str)\n",
    "\n",
    "    for offset_type, feature_classes in features_by_offsets.items():\n",
    "        if isinstance(offset, offset_type):\n",
    "            return [cls() for cls in feature_classes]\n",
    "\n",
    "    supported_freq_msg = f\"\"\"\n",
    "    Unsupported frequency {freq_str}\n",
    "    The following frequencies are supported:\n",
    "        Y   - yearly\n",
    "            alias: A\n",
    "        M   - monthly\n",
    "        W   - weekly\n",
    "        D   - daily\n",
    "        B   - business days\n",
    "        H   - hourly\n",
    "        T   - minutely\n",
    "            alias: min\n",
    "        S   - secondly\n",
    "    \"\"\"\n",
    "    raise RuntimeError(supported_freq_msg)\n",
    "\n",
    "\n",
    "def time_features(dates, freq='h'):\n",
    "    return np.vstack([feat(dates) for feat in time_features_from_frequency_str(freq)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a2dbd2f-c7e7-4e9d-bdfe-5b5a509ed59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from data_provider.timefeatures import time_features\n",
    "\n",
    "class Dataset_Custom(Dataset):\n",
    "    def __init__(self, root_path, flag='train', size=None,\n",
    "                 features='S', data_path='all_countries.csv',\n",
    "                 target='Price (EUR/MWhe)', scale=True, timeenc=0, freq='h'):\n",
    "        # size [seq_len, label_len, pred_len]\n",
    "        # info\n",
    "        if size is None:\n",
    "            self.seq_len = 24 * 4 * 4\n",
    "            self.label_len = 24 * 4\n",
    "            self.pred_len = 24 * 4\n",
    "        else:\n",
    "            self.seq_len = size[0]\n",
    "            self.label_len = size[1]\n",
    "            self.pred_len = size[2]\n",
    "        # init\n",
    "        assert flag in ['train', 'test', 'val']\n",
    "        type_map = {'train': 0, 'val': 1, 'test': 2}\n",
    "        self.set_type = type_map[flag]\n",
    "\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.scale = scale\n",
    "        self.timeenc = timeenc\n",
    "        self.freq = freq\n",
    "\n",
    "        self.root_path = root_path\n",
    "        self.data_path = data_path\n",
    "        self.__read_data__()\n",
    "        self.len = self.__len__()\n",
    "\n",
    "    def __read_data__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        df_raw = pd.read_csv(os.path.join(self.root_path, self.data_path))\n",
    "\n",
    "## encoding country names\n",
    "        country_encoded = pd.get_dummies(df_raw['Country'])\n",
    "        df_raw = pd.concat([df_raw, country_encoded], axis=1)\n",
    "## dropping unnessesary columns\n",
    "        df_raw = df_raw.drop(columns=['ISO3 Code', 'Datetime (Local)', 'Country'])\n",
    "\n",
    "\n",
    "        cols = list(df_raw.columns)\n",
    "        cols.remove(self.target)\n",
    "        cols.remove('Datetime (UTC)')\n",
    "        cols = ['Datetime (UTC)'] + cols + [self.target]\n",
    "        df_raw = df_raw[cols]\n",
    "\n",
    "## defining borders of train, test and validation sets based on the lenght (70% vs. 20% vs. 10%)\n",
    "        num_train = int(len(df_raw) * 0.7)\n",
    "        num_test = int(len(df_raw) * 0.2)\n",
    "        num_vali = len(df_raw) - num_train - num_test\n",
    "        border1s = [0, num_train - self.seq_len, len(df_raw) - num_test - self.seq_len]\n",
    "        border2s = [num_train, num_train + num_vali, len(df_raw)]\n",
    "        border1 = border1s[self.set_type]\n",
    "        border2 = border2s[self.set_type]\n",
    "\n",
    "        if self.features == 'M' or self.features == 'MS':\n",
    "            cols_data = df_raw.columns[1:]\n",
    "            df_data = df_raw[cols_data]\n",
    "        elif self.features == 'S':\n",
    "            df_data = df_raw[[self.target]]\n",
    "\n",
    "        if self.scale:\n",
    "            train_data = df_data[border1s[0]:border2s[0]]\n",
    "            self.scaler.fit(train_data.values)\n",
    "            data = self.scaler.transform(df_data.values)\n",
    "        else:\n",
    "            data = df_data.values\n",
    "\n",
    "        df_stamp = df_raw[['Datetime (UTC)']][border1:border2]\n",
    "        df_stamp['Datetime (UTC)'] = pd.to_datetime(df_stamp['Datetime (UTC)'])\n",
    "        if self.timeenc == 0:\n",
    "            df_stamp['month'] = df_stamp['Datetime (UTC)'].apply(lambda row: row.month)\n",
    "            df_stamp['day'] = df_stamp['Datetime (UTC)'].apply(lambda row: row.day)\n",
    "            df_stamp['weekday'] = df_stamp['Datetime (UTC)'].apply(lambda row: row.weekday())\n",
    "            df_stamp['hour'] = df_stamp['Datetime (UTC)'].apply(lambda row: row.hour)\n",
    "            data_stamp = df_stamp.drop(['Datetime (UTC)'], axis=1).values\n",
    "        elif self.timeenc == 1:\n",
    "            data_stamp = time_features(pd.to_datetime(df_stamp['Datetime (UTC)'].values), freq=self.freq)\n",
    "            data_stamp = data_stamp.transpose(1, 0)\n",
    "\n",
    "        self.data_x = data[border1:border2]\n",
    "        self.data_y = data[border1:border2]\n",
    "        self.data_stamp = data_stamp\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        s_begin = index\n",
    "        s_end = s_begin + self.seq_len\n",
    "        r_begin = s_end - self.label_len\n",
    "        r_end = r_begin + self.label_len + self.pred_len\n",
    "\n",
    "        seq_x = self.data_x[s_begin:s_end]\n",
    "        seq_y = self.data_y[r_begin:r_end]\n",
    "        seq_x_mark = self.data_stamp[s_begin:s_end]\n",
    "        seq_y_mark = self.data_stamp[r_begin:r_end]\n",
    "        \n",
    "        index_list = np.arange(index, index + self.seq_len + self.pred_len, 1)\n",
    "        norm_index = index_list / self.len\n",
    "\n",
    "        return seq_x, seq_y, seq_x_mark, seq_y_mark, norm_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_x) - self.seq_len - self.pred_len + 1\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return self.scaler.inverse_transform(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "101b03c6-566d-41d3-8831-19ba0a277968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from data_provider.data_loader import Dataset_Custom\n",
    "\n",
    "def data_provider(args, flag):\n",
    "    Data = Dataset_Custom\n",
    "    timeenc = 0 if args.embed != 'timeF' else 1\n",
    "\n",
    "    shuffle_flag = True if flag != 'test' else False\n",
    "    drop_last = True\n",
    "    batch_size = args.batch_size\n",
    "    freq = args.freq\n",
    "\n",
    "    data_set = Data(\n",
    "        root_path=args.root_path,\n",
    "        data_path=args.data_path,\n",
    "        flag=flag,\n",
    "        size=[args.seq_len, args.label_len, args.pred_len],\n",
    "        features=args.features,\n",
    "        target=args.target,\n",
    "        timeenc=timeenc,\n",
    "        freq=freq,\n",
    "    )\n",
    "    print(flag, len(data_set))\n",
    "    data_loader = DataLoader(\n",
    "        data_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle_flag,\n",
    "        num_workers=args.num_workers,\n",
    "        drop_last=drop_last)\n",
    "    return data_set, data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5463becb-cf31-487b-96c1-8f862a13d02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.utils.weight_norm as wn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class MLP_bottle(nn.Module):\n",
    "    def __init__(self,input_len,output_len,bottleneck,bias=True):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Sequential(\n",
    "            wn(nn.Linear(input_len, bottleneck,bias=bias)),\n",
    "            nn.ReLU(),\n",
    "            wn(nn.Linear(bottleneck,bottleneck,bias=bias))\n",
    "        )\n",
    "\n",
    "        self.linear2 = nn.Sequential(\n",
    "            wn(nn.Linear(bottleneck, bottleneck)),\n",
    "            nn.ReLU(),\n",
    "            wn(nn.Linear(bottleneck, output_len))\n",
    "        )\n",
    "\n",
    "        self.skip = wn(nn.Linear(input_len, bottleneck,bias=bias))\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.act(self.linear1(x)+self.skip(x))\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class Coefnet(nn.Module):\n",
    "    def __init__(self, blocks,d_model,heads,norm_layer=None, projection=None):\n",
    "        super().__init__()\n",
    "        layers = [BCAB(d_model,heads) for i in range(blocks)]\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.norm = norm_layer\n",
    "        self.projection = projection\n",
    "        # heads = heads if blocks > 0 else 1\n",
    "        self.last_layer = last_layer(d_model,heads)\n",
    "\n",
    "    def forward(self, basis, series):\n",
    "        attns1 = []\n",
    "        attns2 = []\n",
    "        for layer in self.layers:\n",
    "            basis,series,basis_attn,series_attn = layer(basis,series)   #basis(B,N,d)  series(B,C,d)\n",
    "            attns1.append(basis_attn)\n",
    "            attns2.append(series_attn)\n",
    "        \n",
    "        coef = self.last_layer(series,basis)  #(B,k,C,N)\n",
    "        \n",
    "        return coef,attns1,attns2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f27214f0-f848-4698-9e8e-a502bba0fc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.utils.weight_norm as wn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class Basisformer(nn.Module):\n",
    "    def __init__(self,seq_len,pred_len,d_model,heads,basis_nums,block_nums,bottle,map_bottleneck,device,tau):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # model dimentions\n",
    "        self.k = heads # number of attention heads\n",
    "        self.N = basis_nums # number of basis functions\n",
    "        self.coefnet = Coefnet(blocks=block_nums,d_model=d_model,heads=heads)\n",
    "            \n",
    "        self.pred_len = pred_len # prediction length\n",
    "        self.seq_len = seq_len # sequence length\n",
    "\n",
    "        # Multi-Layer Perceptron\n",
    "        self.MLP_x = MLP_bottle(seq_len,heads * int(seq_len/heads),int(seq_len/bottle)) #processes the input sequence length to create a more compact representation\n",
    "        self.MLP_y = MLP_bottle(pred_len,heads * int(pred_len/heads),int(pred_len/bottle)) #same for prediction\n",
    "        self.MLP_sx = MLP_bottle(heads * int(seq_len/heads),seq_len,int(seq_len/bottle)) # re-expands the sequence length helping to restore some structure\n",
    "        self.MLP_sy = MLP_bottle(heads * int(pred_len/heads),pred_len,int(pred_len/bottle)) # same for prediction\n",
    "        \n",
    "        # Linear layers with weight normalization for projecting sequences into the model dimension\n",
    "        self.project1 = wn(nn.Linear(seq_len,d_model))\n",
    "        self.project2 = wn(nn.Linear(seq_len,d_model))\n",
    "        self.project3 = wn(nn.Linear(pred_len,d_model))\n",
    "        self.project4 = wn(nn.Linear(pred_len,d_model))\n",
    "        self.criterion1 = nn.MSELoss()\n",
    "        self.criterion2 = nn.L1Loss(reduction='none')\n",
    "        \n",
    "        self.device = device # setting the device (CPU or GPU)\n",
    "                        \n",
    "        # smooth array\n",
    "        arr = torch.zeros((seq_len+pred_len-2,seq_len+pred_len))\n",
    "        for i in range(seq_len+pred_len-2):\n",
    "            arr[i,i]=-1\n",
    "            arr[i,i+1] = 2\n",
    "            arr[i,i+2] = -1\n",
    "        self.smooth_arr = arr.to(device)\n",
    "\n",
    "        # initializing basis function\n",
    "        # MLP maps input to a higher dim space\n",
    "        self.map_MLP = MLP_bottle(1, # input dim\n",
    "                                  self.N*(self.seq_len+self.pred_len), #output dim\n",
    "                                  map_bottleneck, # hidden layer size for the MLP\n",
    "                                  bias=True) \n",
    "        self.tau = tau # temperature parameter \n",
    "        self.epsilon = 1E-5 # to avoid deletion by zero\n",
    "        \n",
    "    def forward(self,x,mark,y=None,train=True,y_mark=None):\n",
    "        # normalization\n",
    "        mean_x = x.mean(dim=1,keepdim=True)\n",
    "        std_x = x.std(dim=1,keepdim=True)\n",
    "        feature = (x - mean_x) / (std_x + self.epsilon)\n",
    "        # reshaping\n",
    "        B,L,C = feature.shape\n",
    "        feature = feature.permute(0,2,1)\n",
    "        feature = self.project1(feature)   #(B,C,d)\n",
    "        \n",
    "        # creating basis function\n",
    "        m = self.map_MLP( # maps the input marker to a higher-dimensional space\n",
    "            mark[:,0].unsqueeze(1) # selects the first marker and reshapes it for the MLP\n",
    "                         ).reshape(B,self.seq_len + self.pred_len,self.N) #reshapes the output to have other dimensions\n",
    "        \n",
    "        # normalization\n",
    "        m = m / torch.sqrt(torch.sum(m**2,dim=1,keepdim=True)+self.epsilon)\n",
    "        \n",
    "        # using basis functions in the model by splitting and projecting basis functions\n",
    "        raw_m1 = m[:,:self.seq_len].permute(0,2,1)  #(B,L,N) # corresponding to the input sequence\n",
    "        raw_m2 = m[:,self.seq_len:].permute(0,2,1)   #(B,L',N) #corresponding to the prediction sequence\n",
    "        # permute(0,2,1) changes the order of dimensions for compatibility with other operations\n",
    "\n",
    "        m1 = self.project2(raw_m1)    #(B,N,d) projects the input sequence basis functions into the model dimension\n",
    "        \n",
    "        # attention mechanism with basis functions\n",
    "        score,attn_x1,attn_x2 = self.coefnet(m1,feature)    #(B,k,C,N) \n",
    "        # applies the coefficient network to the projected basis functions and the features extracted from the input sequence\n",
    "        # scores represent how much each basis function contributes to the final representation\n",
    "\n",
    "\n",
    "        # combining basis functions\n",
    "        base = self.MLP_y(raw_m2).reshape(B,self.N,self.k,-1).permute(0,2,1,3)   #(B,k,N,L/k)\n",
    "        out = torch.matmul(score,base).permute(0,2,1,3).reshape(B,C,-1)  #(B,C,k * (L/k))\n",
    "        out = self.MLP_sy(out).reshape(B,C,-1).permute(0,2,1)   #（BC,L）\n",
    "        \n",
    "        # reverse normalization\n",
    "        output = out * (std_x + self.epsilon) + mean_x\n",
    "\n",
    "        #loss calculation\n",
    "        if train:\n",
    "            l_smooth = torch.einsum('xl,bln->xbn',self.smooth_arr,m)\n",
    "            l_smooth = abs(l_smooth).mean()\n",
    "            # l_smooth = self.criterion1(l_smooth,torch.zeros_like(l_smooth))\n",
    "            \n",
    "            # #back\n",
    "            mean_y = y.mean(dim=1,keepdim=True)\n",
    "            std_y = y.std(dim=1,keepdim=True)\n",
    "            feature_y_raw = (y - mean_y) / (std_y + self.epsilon)\n",
    "            \n",
    "            feature_y = feature_y_raw.permute(0,2,1)\n",
    "            feature_y = self.project3(feature_y)   #(BC,d)\n",
    "            m2 = self.project4(raw_m2)    #(N,d)\n",
    "            \n",
    "            score_y,attn_y1,attn_y2 = self.coefnet(m2,feature_y)    #(B,k,C,N)\n",
    "            logit_q = score.permute(0,2,3,1) #(B,C,N,k)\n",
    "            logit_k = score_y.permute(0,2,3,1) #(B,C,N,k)\n",
    "\n",
    "            # l_pos = torch.bmm(logit_q.view(-1,1,self.k), logit_k.view(-1,self.k,1)).reshape(-1,1)  #(B*C*N,1,1)\n",
    "            l_neg = torch.bmm(logit_q.reshape(-1,self.N,self.k), logit_k.reshape(-1,self.N,self.k).permute(0,2,1)).reshape(-1,self.N) # (B,C*N,N)\n",
    "\n",
    "            labels = torch.arange(0,self.N,1,dtype=torch.long).unsqueeze(0).repeat(B*C,1).reshape(-1)\n",
    "\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "            cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "            l_entropy = cross_entropy_loss(l_neg/self.tau, labels)           \n",
    "            \n",
    "            return output,l_entropy,l_smooth,attn_x1,attn_x2,attn_y1,attn_y2\n",
    "        else:\n",
    "            #back\n",
    "            mean_y = y.mean(dim=1,keepdim=True)\n",
    "            std_y = y.std(dim=1,keepdim=True)\n",
    "            feature_y_raw = (y - mean_y) / (std_y + self.epsilon)\n",
    "            \n",
    "            feature_y = feature_y_raw.permute(0,2,1)\n",
    "            feature_y = self.project3(feature_y)   #(BC,d)\n",
    "            m2 = self.project4(raw_m2)    #(N,d)\n",
    "            \n",
    "            score_y,attn_y1,attn_y2 = self.coefnet(m2,feature_y)    #(B,k,C,N)\n",
    "            return output,m,attn_x1,attn_x2,attn_y1,attn_y2      \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2d3b15e-cdb7-432a-a22a-c030302f829a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def RSE(pred, true):\n",
    "    return np.sqrt(np.sum((true - pred) ** 2)) / np.sqrt(np.sum((true - true.mean()) ** 2))\n",
    "\n",
    "\n",
    "def CORR(pred, true):\n",
    "    u = ((true - true.mean(0)) * (pred - pred.mean(0))).sum(0)\n",
    "    d = np.sqrt(((true - true.mean(0)) ** 2 * (pred - pred.mean(0)) ** 2).sum(0))\n",
    "    return (u / d).mean(-1)\n",
    "\n",
    "\n",
    "def MAE(pred, true):\n",
    "    return np.mean(np.abs(pred - true))\n",
    "\n",
    "\n",
    "def MSE(pred, true):\n",
    "    return np.mean((pred - true) ** 2)\n",
    "\n",
    "\n",
    "def RMSE(pred, true):\n",
    "    return np.sqrt(MSE(pred, true))\n",
    "\n",
    "\n",
    "def MAPE(pred, true):\n",
    "    return np.mean(np.abs((pred - true) / true))\n",
    "\n",
    "\n",
    "def MSPE(pred, true):\n",
    "    return np.mean(np.square((pred - true) / true))\n",
    "\n",
    "\n",
    "def metric(pred, true):\n",
    "    mae = MAE(pred, true)\n",
    "    mse = MSE(pred, true)\n",
    "    rmse = RMSE(pred, true)\n",
    "    mape = MAPE(pred, true)\n",
    "    mspe = MSPE(pred, true)\n",
    "\n",
    "    return mae, mse, rmse, mape, mspe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd24c1ec-0d34-40ed-8923-19d3077a7536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "from data_provider.data_factory import data_provider\n",
    "from torch import optim\n",
    "from model import Basisformer\n",
    "from torch import nn\n",
    "import time\n",
    "import numpy as np\n",
    "from evaluate_tool import metric\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pyplot import plot_seq_feature\n",
    "from adabelief_pytorch import AdaBelief\n",
    "import logging\n",
    "import random\n",
    "\n",
    "\n",
    "def vali(vali_data, vali_loader, criterion, epoch, writer, flag='vali'):\n",
    "    total_loss = []\n",
    "    model.eval()\n",
    "    count_error = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark,index) in enumerate(vali_loader):\n",
    "            batch_x = batch_x.float().to(device)\n",
    "            batch_y = batch_y.float()\n",
    "\n",
    "            f_dim = -1 if args.features == 'MS' else 0\n",
    "            origin = batch_y[:, :args.seq_len, f_dim:].to(device)\n",
    "            batch_y = batch_y[:, -args.pred_len:, f_dim:].to(device)\n",
    "            batch_y_mark = batch_y_mark.float().to(device)\n",
    "            \n",
    "            real_batch_x = batch_x\n",
    "            \n",
    "            outputs,m,attn_x1,attn_x2,attn_y1,attn_y2 = model(batch_x,index.float().to(device),batch_y,train=False,y_mark=batch_y_mark)\n",
    "            \n",
    "            pred = outputs.detach().cpu()\n",
    "            true = batch_y.detach().cpu()\n",
    "\n",
    "            loss_raw = criterion(pred, true)\n",
    "            loss = loss_raw.mean()\n",
    "\n",
    "            total_loss.append(loss)\n",
    "\n",
    "            if i == 0:\n",
    "                fig = plot_seq_feature(outputs, batch_y, real_batch_x, flag)\n",
    "                writer.add_figure(\"figure_{}\".format(flag), fig, global_step=epoch)\n",
    "                    \n",
    "    total_loss = np.average(total_loss)\n",
    "        \n",
    "    model.train()\n",
    "    return total_loss\n",
    "\n",
    "def train():\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    log_and_print('[Info] Number of parameters: {}'.format(num_params))\n",
    "\n",
    "    # data sets and their corresponding loaders\n",
    "    train_set, train_loader = data_provider(args, \"train\")\n",
    "    vali_data, vali_loader = data_provider(args,flag='val')\n",
    "    test_data, test_loader = data_provider(args,flag='test')\n",
    "    \n",
    "\n",
    "    para1 = [param for name,param in model.named_parameters() if 'map_MLP' in name]\n",
    "    para2 = [param for name,param in model.named_parameters() if 'map_MLP' not in name]\n",
    "\n",
    "    # optimizer updates the model parameters during training\n",
    "    # optimizer = AdaBelief(model.parameters(), lr=args.learning_rate, eps=1e-16, betas=(0.9,0.999), weight_decouple = True, rectify = False) \n",
    "    optimizer = AdaBelief([{'params':para1,'lr':5e-3},{'params':para2,'lr':args.learning_rate}], eps=1e-16, betas=(0.9,0.999), weight_decouple = True, rectify = False) \n",
    "    # optimizer = AdaBelief(model.parameters(), lr=args.learning_rate, eps=1e-16, betas=(0.9,0.999), weight_decouple = True, rectify = False) \n",
    "    \n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    criterion_view = nn.MSELoss(reduction='none')\n",
    "\n",
    "    # number of batches in the training set?\n",
    "    train_steps = len(train_loader)\n",
    "    # initializing the Tensor Board writer for logging training process\n",
    "    writer = SummaryWriter(os.path.join(record_dir,'event'))\n",
    "\n",
    "    # defining for early stopping\n",
    "    best_loss = 0\n",
    "    count_error = 0\n",
    "    count = 0\n",
    "    \n",
    "\n",
    "    #training loop\n",
    "\n",
    "    for epoch in range(args.train_epochs):\n",
    "        #lists to store\n",
    "        train_loss = []\n",
    "        loss_pred = []\n",
    "        loss_of_ce = []\n",
    "        l_s = []\n",
    "        #setting model to training mode\n",
    "        model.train()\n",
    "        epoch_time = time.time()\n",
    "\n",
    "        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark,index) in enumerate(train_loader):\n",
    "            #clears the gradients of all optimized tensors\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # loading data to the specified device (originally to cuda)\n",
    "            batch_x = batch_x.float().to(device) # (B,L,C)\n",
    "            batch_y = batch_y.float().to(device) # (B,L,C)\n",
    "            batch_y_mark = batch_y_mark.float().to(device)\n",
    "            \n",
    "            #feature dimension\n",
    "            f_dim = -1 if args.features == 'MS' else 0\n",
    "            #matching the target sequence length required by the model\n",
    "            batch_y = batch_y[:, -args.pred_len:, f_dim:].to(device)\n",
    "\n",
    "            #forward pass through the model to get outputs and losses\n",
    "            outputs,loss_infonce,loss_smooth,attn_x1,attn_x2,attn_y1,attn_y2 = model(batch_x,index.float().to(device),batch_y,y_mark=batch_y_mark)\n",
    "            \n",
    "            # calculating loss\n",
    "            loss_p = criterion(outputs, batch_y)\n",
    "            lam1 = args.loss_weight_prediction\n",
    "            lam2 = args.loss_weight_infonce\n",
    "            lam3 = args.loss_weight_smooth\n",
    "        \n",
    "            # if loss_p > 5:\n",
    "            #     count_error = count_error +1\n",
    "            #     writer.add_scalar('error_loss', loss_p, global_step=count_error)\n",
    "            #     fig = plot_seq_feature(outputs, batch_y,batch_x,error=True,input=batch_x)\n",
    "            #     writer.add_figure(\"figure_error\", fig, global_step=count_error)\n",
    "            #     log_and_print(loss_p)\n",
    "\n",
    "            # total loss  \n",
    "            loss = lam1 * loss_p + lam2 * loss_infonce  + lam3 * loss_smooth\n",
    "            train_loss.append(loss.item())\n",
    "            loss_pred.append(loss_p.item())\n",
    "            loss_of_ce.append(loss_infonce.item())\n",
    "            l_s.append(loss_smooth.item())\n",
    "\n",
    "            # greadient of the loss\n",
    "            loss.backward()\n",
    "\n",
    "            #updating model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            #logging every fifth step of the training process \n",
    "            if (i+1) % (train_steps//5) == 0:\n",
    "                log_and_print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n",
    "\n",
    "        # every epoch logging\n",
    "        log_and_print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n",
    "        # losses of every epoch\n",
    "        train_loss = np.average(train_loss)\n",
    "        loss1 = np.average(loss_pred)\n",
    "        log_and_print('loss_pred:{0}'.format(loss1))\n",
    "        loss2 = np.average(loss_of_ce)\n",
    "        log_and_print('loss entropy:{0}'.format(loss2))\n",
    "        loss3 = np.average(l_s)\n",
    "        log_and_print('loss smooth:{0}'.format(loss3))\n",
    "        vali_loss = vali(vali_data, vali_loader, criterion_view, epoch, writer, 'vali')\n",
    "        test_loss = vali(test_data, test_loader, criterion_view, epoch, writer, 'test')\n",
    "        # logging\n",
    "        log_and_print(\"Epoch: {0} | Train Loss: {1:.7f} Vali Loss: {2:.7f} Test Loss: {3:.7f}\".format(\n",
    "            epoch + 1, train_loss, vali_loss, test_loss))\n",
    "\n",
    "        # figures to TensorBoard\n",
    "        fig = plot_seq_feature(outputs, batch_y, batch_x)\n",
    "        writer.add_figure(\"figure_train\", fig, global_step=epoch)\n",
    "        writer.add_scalar('train_loss', train_loss, global_step=epoch)\n",
    "        writer.add_scalar('vali_loss', vali_loss, global_step=epoch)\n",
    "        writer.add_scalar('test_loss', test_loss, global_step=epoch)\n",
    "        \n",
    "        #saving model chaeckpoints\n",
    "        ckpt_path = os.path.join(record_dir,args.check_point)\n",
    "        if not os.path.exists(ckpt_path):\n",
    "            os.makedirs(ckpt_path)\n",
    "        #saving in new folder if it is firs tepoch\n",
    "        if best_loss == 0:\n",
    "            best_loss = vali_loss\n",
    "            torch.save(model.state_dict(), os.path.join(ckpt_path, 'valid_best_checkpoint.pth'))\n",
    "        else:\n",
    "            if vali_loss < best_loss: #updates the results if vali loss improves\n",
    "                torch.save(model.state_dict(), os.path.join(ckpt_path, 'valid_best_checkpoint.pth'))\n",
    "                best_loss = vali_loss\n",
    "                count = 0\n",
    "            else:\n",
    "                count = count + 1\n",
    "        #final save at the end of each epoch\n",
    "        torch.save(model.state_dict(), os.path.join(ckpt_path, 'final_checkpoint.pth'))\n",
    "        #stopping training if loss doesn't improve for a number of epochs\n",
    "        if count >= args.patience:\n",
    "            break\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aad1ab79-d58d-4f47-bd20-912a81f03ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_and_print(text):\n",
    "    logging.info(text)\n",
    "    print(text)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c9cbd1d-dcae-40c6-92ed-ae8e8d3090eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "Namespace(N=10, batch_size=32, block_nums=2, bottleneck=2, check_point='checkpoint', d_model=100, data='custom', data_path='all_countries.csv', device=0, embed='timeF', features='M', freq='h', heads=16, is_training=True, label_len=96, learning_rate=0.0005, loss_weight_infonce=1.0, loss_weight_prediction=1.0, loss_weight_smooth=1.0, map_bottleneck=20, num_workers=0, patience=3, pred_len=96, root_path='data', seq_len=96, target='Price (EUR/MWhe)', tau=0.07, train_epochs=1)\n",
      "Basisformer(\n",
      "  (coefnet): Coefnet(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x BCAB(\n",
      "        (cross_attention_basis): channel_AutoCorrelationLayer(\n",
      "          (query_projection): Linear(in_features=100, out_features=96, bias=True)\n",
      "          (key_projection): Linear(in_features=100, out_features=96, bias=True)\n",
      "          (value_projection): Linear(in_features=100, out_features=96, bias=True)\n",
      "          (out_projection): Linear(in_features=96, out_features=100, bias=True)\n",
      "          (attend): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (conv1_basis): Linear(in_features=100, out_features=400, bias=True)\n",
      "        (conv2_basis): Linear(in_features=400, out_features=100, bias=True)\n",
      "        (dropout_basis): Dropout(p=0.1, inplace=False)\n",
      "        (cross_attention_ts): channel_AutoCorrelationLayer(\n",
      "          (query_projection): Linear(in_features=100, out_features=96, bias=True)\n",
      "          (key_projection): Linear(in_features=100, out_features=96, bias=True)\n",
      "          (value_projection): Linear(in_features=100, out_features=96, bias=True)\n",
      "          (out_projection): Linear(in_features=96, out_features=100, bias=True)\n",
      "          (attend): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (conv1_ts): Linear(in_features=100, out_features=400, bias=True)\n",
      "        (conv2_ts): Linear(in_features=400, out_features=100, bias=True)\n",
      "        (dropout_ts): Dropout(p=0.1, inplace=False)\n",
      "        (layer_norm11): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "        (layer_norm12): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "        (layer_norm21): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "        (layer_norm22): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (last_layer): last_layer(\n",
      "      (query_projection): Linear(in_features=100, out_features=96, bias=True)\n",
      "      (key_projection): Linear(in_features=100, out_features=96, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (MLP_x): MLP_bottle(\n",
      "    (linear1): Sequential(\n",
      "      (0): Linear(in_features=96, out_features=48, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=48, out_features=48, bias=True)\n",
      "    )\n",
      "    (linear2): Sequential(\n",
      "      (0): Linear(in_features=48, out_features=48, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=48, out_features=96, bias=True)\n",
      "    )\n",
      "    (skip): Linear(in_features=96, out_features=48, bias=True)\n",
      "    (act): ReLU()\n",
      "  )\n",
      "  (MLP_y): MLP_bottle(\n",
      "    (linear1): Sequential(\n",
      "      (0): Linear(in_features=96, out_features=48, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=48, out_features=48, bias=True)\n",
      "    )\n",
      "    (linear2): Sequential(\n",
      "      (0): Linear(in_features=48, out_features=48, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=48, out_features=96, bias=True)\n",
      "    )\n",
      "    (skip): Linear(in_features=96, out_features=48, bias=True)\n",
      "    (act): ReLU()\n",
      "  )\n",
      "  (MLP_sx): MLP_bottle(\n",
      "    (linear1): Sequential(\n",
      "      (0): Linear(in_features=96, out_features=48, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=48, out_features=48, bias=True)\n",
      "    )\n",
      "    (linear2): Sequential(\n",
      "      (0): Linear(in_features=48, out_features=48, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=48, out_features=96, bias=True)\n",
      "    )\n",
      "    (skip): Linear(in_features=96, out_features=48, bias=True)\n",
      "    (act): ReLU()\n",
      "  )\n",
      "  (MLP_sy): MLP_bottle(\n",
      "    (linear1): Sequential(\n",
      "      (0): Linear(in_features=96, out_features=48, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=48, out_features=48, bias=True)\n",
      "    )\n",
      "    (linear2): Sequential(\n",
      "      (0): Linear(in_features=48, out_features=48, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=48, out_features=96, bias=True)\n",
      "    )\n",
      "    (skip): Linear(in_features=96, out_features=48, bias=True)\n",
      "    (act): ReLU()\n",
      "  )\n",
      "  (project1): Linear(in_features=96, out_features=100, bias=True)\n",
      "  (project2): Linear(in_features=96, out_features=100, bias=True)\n",
      "  (project3): Linear(in_features=96, out_features=100, bias=True)\n",
      "  (project4): Linear(in_features=96, out_features=100, bias=True)\n",
      "  (criterion1): MSELoss()\n",
      "  (criterion2): L1Loss()\n",
      "  (map_MLP): MLP_bottle(\n",
      "    (linear1): Sequential(\n",
      "      (0): Linear(in_features=1, out_features=20, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=20, out_features=20, bias=True)\n",
      "    )\n",
      "    (linear2): Sequential(\n",
      "      (0): Linear(in_features=20, out_features=20, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=20, out_features=1920, bias=True)\n",
      "    )\n",
      "    (skip): Linear(in_features=1, out_features=20, bias=True)\n",
      "    (act): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ekaterinabasova/miniconda3/envs/basisformer_x86_env/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='Time series prediction - Basisformer')\n",
    "    parser.add_argument('--is_training', type=bool, default=True, help='train or test')\n",
    "    parser.add_argument('--device', type=int, default=0, help='gpu dvice')\n",
    "\n",
    "    # data loader\n",
    "    parser.add_argument('--num_workers', type=int, default=0, help='data loader num workers')\n",
    "    parser.add_argument('--data', type=str, default='custom', help='dataset type')\n",
    "    parser.add_argument('--root_path', type=str, default='data', help='root path of the data file')\n",
    "    parser.add_argument('--data_path', type=str, default='all_countries.csv', help='data file')\n",
    "    parser.add_argument('--features', type=str, default='M',\n",
    "                        help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, '\n",
    "                            'S:univariate predict univariate, MS:multivariate predict univariate')\n",
    "    parser.add_argument('--target', type=str, default='Price (EUR/MWhe)', help='target feature in S or MS task')\n",
    "    parser.add_argument('--freq', type=str, default='h',\n",
    "                    help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, '\n",
    "                         'b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "\n",
    "    # forecasting task\n",
    "    parser.add_argument('--seq_len', type=int, default=96, help='input sequence length')\n",
    "    parser.add_argument('--label_len', type=int, default=96, help='start token length')\n",
    "    parser.add_argument('--pred_len', type=int, default=96, help='prediction sequence length')\n",
    "    # parser.add_argument('--cross_activation', type=str default='tanh'\n",
    "\n",
    "    # model define\n",
    "    parser.add_argument('--embed', type=str, default='timeF',\n",
    "                        help='time features encoding, options:[timeF, fixed, learned]')\n",
    "    parser.add_argument('--heads', type=int, default=16, help='head in attention')\n",
    "    parser.add_argument('--d_model', type=int, default=100, help='dimension of model')\n",
    "    parser.add_argument('--N', type=int, default=10, help='number of learnable basis')\n",
    "    parser.add_argument('--block_nums', type=int, default=2, help='number of blocks')\n",
    "    parser.add_argument('--bottleneck', type=int, default=2, help='reduction of bottleneck')\n",
    "    parser.add_argument('--map_bottleneck', type=int, default=20, help='reduction of mapping bottleneck')\n",
    "\n",
    "    # optimization\n",
    "    parser.add_argument('--train_epochs', type=int, default=1, help='train epochs')\n",
    "    parser.add_argument('--batch_size', type=int, default=32, help='batch size of train input data')\n",
    "    parser.add_argument('--patience', type=int, default=1, help='early stopping patience')\n",
    "    parser.add_argument('--learning_rate', type=float, default=5e-4, help='optimizer learning rate')\n",
    "    parser.add_argument('--tau', type=float, default=0.07, help='temperature of infonce loss')\n",
    "    parser.add_argument('--loss_weight_prediction', type=float, default=1.0, help='weight of prediction loss')\n",
    "    parser.add_argument('--loss_weight_infonce', type=float, default=1.0, help='weight of infonce loss')\n",
    "    parser.add_argument('--loss_weight_smooth', type=float, default=1.0, help='weight of smooth loss')\n",
    "\n",
    "\n",
    "    #checkpoint_path\n",
    "    parser.add_argument('--check_point',type=str,default='checkpoint',help='check point path, relative path')\n",
    "    \n",
    "    #args = parser.parse_args()\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    \n",
    "    record_dir = os.path.join('records',args.data_path.split('.')[0],'features_'+args.features,\\\n",
    "                              'seq_len'+str(args.seq_len)+','+'pred_len'+str(args.pred_len))\n",
    "    if not os.path.exists(record_dir):\n",
    "        os.makedirs(record_dir)\n",
    "    \n",
    "    if args.is_training:\n",
    "        logger_file = os.path.join(record_dir,'train.log')\n",
    "    else:\n",
    "        logger_file = os.path.join(record_dir,'test.log')\n",
    "        \n",
    "    if os.path.exists(logger_file):\n",
    "        with open(logger_file, \"w\") as file:\n",
    "            file.truncate(0)\n",
    "    logging.basicConfig(filename=logger_file, level=logging.INFO)\n",
    "    \n",
    "    log_and_print('Args in experiment:')\n",
    "    log_and_print(args)\n",
    "\n",
    "    device = init_dl_program(args.device, seed=0,max_threads=8) if torch.cuda.is_available() else \"cpu\"\n",
    "    model = Basisformer(args.seq_len,args.pred_len,args.d_model,args.heads,args.N,args.block_nums,args.bottleneck,args.map_bottleneck,device,args.tau)\n",
    "\n",
    "    log_and_print(model)\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d34df7-21cb-4303-87f9-d5ec8c03ec8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining new parameter values\n",
    "sys.argv = ['--batch_size=16', \n",
    "            '--seq_len=24', \n",
    "            '--label_len=24', \n",
    "            '--pred_len=24', \n",
    "            '--train_epochs=1', \n",
    "            '--heads=2', \n",
    "            '--d_model=4', \n",
    "            '--block_nums=1', \n",
    "            '--N=5', \n",
    "            '--bottleneck=1', \n",
    "            '--map_bottleneck=10']\n",
    "## parsing new values\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "## initializing device and model\n",
    "device = init_dl_program(args.device, seed=0, max_threads=8) if torch.cuda.is_available() else \"cpu\"\n",
    "model = Basisformer(args.seq_len, args.pred_len, args.d_model, args.heads, args.N, args.block_nums, args.bottleneck, args.map_bottleneck, device, args.tau)\n",
    "\n",
    "## starting\n",
    "if args.is_training:\n",
    "    train()\n",
    "else:\n",
    "    test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (basisformer_x86_env)",
   "language": "python",
   "name": "basisformer_x86_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
